{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "88bf173d-f24b-401e-bdc5-68356cd84d26",
   "metadata": {},
   "source": [
    "# Pipeline to query, download and chip HLS and CDL layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c8cde70-7cf0-49c8-a69e-3f2a45757876",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas\n",
    "import json\n",
    "import xarray\n",
    "import rasterio\n",
    "import rioxarray\n",
    "import os\n",
    "import fiona\n",
    "import urllib.request as urlreq\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import xmltodict\n",
    "import shutil\n",
    "import datetime\n",
    "import boto3\n",
    "import pyproj\n",
    "import multiprocessing as mp\n",
    "\n",
    "from pystac_client import Client \n",
    "from collections import defaultdict\n",
    "from glob import glob\n",
    "from rasterio.enums import Resampling\n",
    "import matplotlib.pyplot as plt\n",
    "from subprocess import Popen, PIPE\n",
    "from tqdm import tqdm\n",
    "from netrc import netrc\n",
    "from platform import system\n",
    "from getpass import getpass\n",
    "from rasterio.session import AWSSession\n",
    "from pathlib import Path\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a6db73-e1af-470e-8a13-e923fcd74fc0",
   "metadata": {},
   "source": [
    "### Setting folder paths and file paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9f65500-9fc0-4b7f-8e4f-46c40a079acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "cloud_thres = 5 # percent cloud cover for tile level query\n",
    "\n",
    "root_path = \"/data/\"\n",
    "req_path = \"/home/cdl_training_data/data/\"\n",
    "extra_files = \"/data/requirements/\"\n",
    "\n",
    "## file paths\n",
    "chip_file =  req_path + \"chip_bbox.geojson\"\n",
    "chipping_json = req_path + \"chip_bbox_5070.geojson\"\n",
    "chip_csv = req_path + \"chip_tracker.csv\"\n",
    "kml_file = extra_files + 'sentinel_tile_grid.kml'\n",
    "# tile_tracker_csv = req_path + \"tile_tracker.csv\"\n",
    "cdl_file = extra_files + \"2022_30m_cdls.tif\"\n",
    "cdl_reclass_csv = req_path + \"cdl_total_dst.csv\"\n",
    "\n",
    "## folder paths\n",
    "chip_dir = root_path + 'chips/'\n",
    "tile_dir = root_path + 'tiles/'\n",
    "chip_dir_filt = root_path + 'chips_filtered/'\n",
    "chip_fmask_dir = root_path + 'chips_fmask/'\n",
    "\n",
    "## Create necessary folders\n",
    "if not os.path.exists(chip_dir):\n",
    "   os.makedirs(chip_dir)\n",
    "if not os.path.exists(tile_dir):\n",
    "   os.makedirs(tile_dir)\n",
    "if not os.path.exists(chip_dir_filt):\n",
    "   os.makedirs(chip_dir_filt)\n",
    "if not os.path.exists(chip_fmask_dir):\n",
    "   os.makedirs(chip_fmask_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da2291ef-8954-45fe-9d77-41048a58ecaa",
   "metadata": {},
   "source": [
    "### Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "572aad79-35fe-4640-85a0-a489fa18f09e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading chips bounding boxes from geojson\n",
    "with open(chip_file, \"r\") as file:\n",
    "    chips = json.load(file)\n",
    "    # print(chips)\n",
    "\n",
    "# Create lists about chip information to find tiles corresponding to it later\n",
    "chip_ids = []\n",
    "chip_x = []\n",
    "chip_y = []\n",
    "for item in chips['features']:\n",
    "    chip_ids.append(item['properties']['id'])\n",
    "    chip_x.append(item['properties']['center'][0])\n",
    "    chip_y.append(item['properties']['center'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d739262-045a-4ed9-8930-9dd5ea203e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/data/chip_ids.json\", \"w\") as f:\n",
    "    json.dump(chip_ids, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a56e07ac-9b0a-4d7c-bad1-51586770a1f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in sentinel kml file\n",
    "fiona.drvsupport.supported_drivers['KML'] = 'rw'\n",
    "tile_src = geopandas.read_file(kml_file, driver='KML')\n",
    "\n",
    "# Create table containing information about sentinel tiles\n",
    "tile_name = []\n",
    "tile_x = []\n",
    "tile_y = []\n",
    "for tile_ind in range(tile_src.shape[0]):\n",
    "    tile_name.append(tile_src.iloc[tile_ind].Name)\n",
    "    tile_x.append(tile_src.iloc[tile_ind].geometry.centroid.x)\n",
    "    tile_y.append(tile_src.iloc[tile_ind].geometry.centroid.y)\n",
    "tile_name = np.array(tile_name)\n",
    "tile_x = np.array(tile_x)\n",
    "tile_y = np.array(tile_y)\n",
    "tile_src = pd.concat([tile_src, tile_src.bounds], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "618cc2a5-678c-47fd-a324-0d89e8f74e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_tile(x,y):\n",
    "    \"\"\"\n",
    "    Identify closest tile\n",
    "    \"\"\"\n",
    "    \n",
    "    s = (tile_x - x)**2+(tile_y - y)**2\n",
    "    tname = tile_name[np.argmin(s)]\n",
    "    return(tname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "923bcea3-efa4-41fc-90ef-0877dd270270",
   "metadata": {},
   "outputs": [],
   "source": [
    "chip_df = pd.DataFrame({\"chip_id\" : chip_ids, \"chip_x\" : chip_x, \"chip_y\" : chip_y})\n",
    "chip_df['tile'] = chip_df.apply(lambda row : find_tile(row['chip_x'], row['chip_y']), axis = 1)\n",
    "chip_df.tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be986eb5-6c6e-4834-8fa5-dc36134e7839",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save dataframe to csv for later uses\n",
    "chip_df.to_csv(req_path + \"chip_df.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b451da-cf5f-4c2b-9531-15fa20121ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "chip_df = pd.read_csv(req_path + \"chip_df.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca55cff-659f-4d68-a894-278af32fb9a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tiles = chip_df.tile.unique().tolist()\n",
    "tiles[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8575471-42cb-47e2-86c3-15068d840868",
   "metadata": {},
   "source": [
    "### Querying tile links based on geometry of chips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8679f41b-87b1-4b3d-a2f2-559b9df3fdfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "STAC_URL = 'https://cmr.earthdata.nasa.gov/stac'\n",
    "catalog = Client.open(f'{STAC_URL}/LPCLOUD/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d6080c-87aa-4ae8-bcee-71e9757b3dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tile-level 5% cloud cover threshold query\n",
    "# tile_list = []\n",
    "print(f\"There are a total of {len(tiles)} tiles\")\n",
    "tile_iter = 0\n",
    "for current_tile in tiles:\n",
    "\n",
    "    chip_df_filt = chip_df.loc[chip_df.tile == current_tile]#.reset_index()\n",
    "    first_chip_id = chip_df_filt.chip_id.iloc[0]\n",
    "    first_chip_index_in_json = chip_ids.index(first_chip_id)\n",
    "    roi = chips['features'][first_chip_index_in_json]['geometry']\n",
    "\n",
    "    search = catalog.search(\n",
    "        collections = ['HLSS30.v2.0'],\n",
    "        intersects = roi,\n",
    "        datetime = '2022-03-01/2022-09-30',\n",
    "    ) \n",
    "    \n",
    "    num_results = search.matched()\n",
    "    item_collection = search.get_all_items()\n",
    "    \n",
    "    tile_name = \"T\" + current_tile\n",
    "    iter_items = 0\n",
    "    for i in tqdm(item_collection ,desc=f\"({tile_iter}/{len(tiles)})\"):\n",
    "        if i.id.split('.')[2] == tile_name:\n",
    "            if i.properties['eo:cloud_cover'] <= cloud_thres:\n",
    "                response = requests.get(i.assets['metadata'].href)\n",
    "                if response.status_code == 200:\n",
    "                    temp_xml = response.text\n",
    "                    temp_xml = xmltodict.parse(temp_xml)\n",
    "                    temp_dict = {\"tile_id\": tile_name, \"cloud_cover\": i.properties['eo:cloud_cover'],\n",
    "                                 \"date\": datetime.datetime.strptime(i.properties['datetime'].split('T')[0], \"%Y-%m-%d\"), \n",
    "                                 \"spatial_cover\": int(temp_xml['Granule']['AdditionalAttributes']['AdditionalAttribute'][3]['Values']['Value']),\n",
    "                                 \"http_links\": {\"B02\": i.assets['B02'].href, \"B03\": i.assets['B03'].href, \"B04\": i.assets['B04'].href,  \"B8A\": i.assets['B8A'].href,\n",
    "                                                \"B11\": i.assets['B11'].href, \"B12\": i.assets['B12'].href, \"Fmask\": i.assets['Fmask']},\n",
    "                                \"s3_links\": {\"B02\": i.assets['B02'].href.replace('https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/', 's3:/'), \n",
    "                                             \"B03\": i.assets['B03'].href.replace('https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/', 's3:/'), \n",
    "                                             \"B04\": i.assets['B04'].href.replace('https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/', 's3:/'), \n",
    "                                             \"B8A\": i.assets['B8A'].href.replace('https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/', 's3:/'),\n",
    "                                             \"B11\": i.assets['B11'].href.replace('https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/', 's3:/'),\n",
    "                                             \"B12\": i.assets['B12'].href.replace('https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/', 's3:/'),\n",
    "                                             \"Fmask\": i.assets['Fmask'].href.replace('https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/', 's3:/')}}\n",
    "                    tile_list.append(temp_dict)\n",
    "                    iter_items += 1\n",
    "                else: \n",
    "                    assert False, f\"Failed to fetch XML from {i.assets['metadata'].href}. Error code: {response.status_code}\"\n",
    "            \n",
    "    tile_iter += 1\n",
    "    \n",
    "tile_df = pd.DataFrame(tile_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7431fc8b-0f64-408b-8500-24a1bae5aa01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to csv for later uses\n",
    "tile_df.to_csv(req_path + \"tile_df.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a1fbb28-c2ec-41f1-bc60-c0c18b358726",
   "metadata": {},
   "outputs": [],
   "source": [
    "tile_df = pd.read_csv(req_path + \"tile_df.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf8f2617-3d4a-4651-92ba-6d82f4e39f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "tile_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c8a157d-9e4a-4a79-b98e-d94e8f1916bf",
   "metadata": {},
   "source": [
    "### Filtering based on spatial coverage of the tiles we gathered earlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59767242-2158-4cd7-bdc7-1719207f0552",
   "metadata": {},
   "outputs": [],
   "source": [
    "def spatial_filtering (dataframe):\n",
    "    \"\"\"\n",
    "        Using spatial coverage percentage to filter chips\n",
    "\n",
    "        Args:\n",
    "            dataframe: A pandas dataframe that generated previously\n",
    "    \"\"\"\n",
    "    cover_list = [100, 90, 80, 70, 60, 50]\n",
    "    tile_list_ft = []\n",
    "    tile_list = dataframe.tile_id.unique().tolist()\n",
    "    \n",
    "    for tile in tqdm(tile_list):\n",
    "        temp_df = dataframe[dataframe.tile_id == tile]\n",
    "        for cover_pct in cover_list:\n",
    "            \n",
    "            temp_df_filtered = temp_df[temp_df.spatial_cover >= cover_pct]\n",
    "            if len(temp_df_filtered) >= 3:\n",
    "                for i in range(len(temp_df_filtered)):\n",
    "                    tile_list_ft.append(temp_df_filtered.iloc[i])\n",
    "                break\n",
    "    \n",
    "    tile_df_filtered = pd.DataFrame(tile_list_ft)\n",
    "    return tile_df_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca6941b5-9241-4626-bbec-56118c91ba95",
   "metadata": {},
   "outputs": [],
   "source": [
    "cover_df = spatial_filtering(tile_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b02b75c-55cc-43a1-a6d9-913771eb3043",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_scenes(dataframe):\n",
    "    \"\"\"\n",
    "        Selecting best spatial covered scenes based on timesteps\n",
    "\n",
    "        Args:\n",
    "            dataframe: A pandas dataframe that generated previously\n",
    "    \"\"\"\n",
    "    select_tiles = []\n",
    "    tile_list = dataframe.tile_id.unique().tolist()\n",
    "\n",
    "    for tile in tqdm(tile_list):\n",
    "        temp_df = dataframe[dataframe.tile_id == tile].sort_values('date').reset_index(drop=True)\n",
    "        select_tiles.extend([temp_df.iloc[0], temp_df.iloc[len(temp_df) // 2], temp_df.iloc[-1]])\n",
    "\n",
    "    return pd.DataFrame(select_tiles).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b23da90c-e163-45fc-9ea6-ef8e3130bc95",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_tiles = select_scenes(cover_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7caff71-b5be-4d54-be44-a0938bd55159",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_tiles.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "264fae95-6630-46e0-bd05-83f866cd7a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to csv for later uses\n",
    "selected_tiles.to_csv(req_path + \"selected_tiles.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fc0ddb0-da8d-4fa7-8de3-241e70cacbe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_tiles = pd.read_csv(req_path + \"selected_tiles.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3221c2e7-ed10-412e-8b12-fd398875aafc",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_tiles.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e34d499-1a6e-483f-9f19-8daa6b1dd4c3",
   "metadata": {},
   "source": [
    "### Data downloading\n",
    "\n",
    "Creating netrc file on root for credentials (Run Once each session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38664ffa-cd57-4f1b-a7f3-cdb63554ad94",
   "metadata": {},
   "outputs": [],
   "source": [
    "urs = 'urs.earthdata.nasa.gov'    # Earthdata URL endpoint for authentication\n",
    "prompts = ['Enter NASA Earthdata Login Username: ',\n",
    "           'Enter NASA Earthdata Login Password: ']\n",
    "\n",
    "# Determine the OS (Windows machines usually use an '_netrc' file)\n",
    "netrc_name = \"_netrc\" if system()==\"Windows\" else \".netrc\"\n",
    "\n",
    "# Determine if netrc file exists, and if so, if it includes NASA Earthdata Login Credentials\n",
    "try:\n",
    "    netrcDir = os.path.expanduser(f\"~/{netrc_name}\")\n",
    "    netrc(netrcDir).authenticators(urs)[0]\n",
    "\n",
    "# Below, create a netrc file and prompt user for NASA Earthdata Login Username and Password\n",
    "except FileNotFoundError:\n",
    "    homeDir = os.path.expanduser(\"~\")\n",
    "    Popen('touch {0}{2} | echo machine {1} >> {0}{2}'.format(homeDir + os.sep, urs, netrc_name), shell=True)\n",
    "    Popen('echo login {} >> {}{}'.format(getpass(prompt=prompts[0]), homeDir + os.sep, netrc_name), shell=True)\n",
    "    Popen('echo \\'password {} \\'>> {}{}'.format(getpass(prompt=prompts[1]), homeDir + os.sep, netrc_name), shell=True)\n",
    "    # Set restrictive permissions\n",
    "    Popen('chmod 0600 {0}{1}'.format(homeDir + os.sep, netrc_name), shell=True)\n",
    "\n",
    "    # Determine OS and edit netrc file if it exists but is not set up for NASA Earthdata Login\n",
    "except TypeError:\n",
    "    homeDir = os.path.expanduser(\"~\")\n",
    "    Popen('echo machine {1} >> {0}{2}'.format(homeDir + os.sep, urs, netrc_name), shell=True)\n",
    "    Popen('echo login {} >> {}{}'.format(getpass(prompt=prompts[0]), homeDir + os.sep, netrc_name), shell=True)\n",
    "    Popen('echo \\'password {} \\'>> {}{}'.format(getpass(prompt=prompts[1]), homeDir + os.sep, netrc_name), shell=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aace234-4fba-4cb2-adb6-8e17601b067c",
   "metadata": {},
   "source": [
    "### Getting temporary credentials for NASA's S3 Bucket(Run once every 1 hrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fadfd603-27d5-4354-912a-f5f7ebdb799f",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_cred_endpoint = 'https://data.lpdaac.earthdatacloud.nasa.gov/s3credentials'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b5232e-2040-40de-8915-875c13925ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_temp_creds():\n",
    "    temp_creds_url = s3_cred_endpoint\n",
    "    return requests.get(temp_creds_url).json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eadb6ff-5b4a-4579-bbef-a3a3e2b8212a",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_creds_req = get_temp_creds()\n",
    "\n",
    "boto3_session = boto3.Session(aws_access_key_id=temp_creds_req['accessKeyId'], \n",
    "                            aws_secret_access_key=temp_creds_req['secretAccessKey'],\n",
    "                            aws_session_token=temp_creds_req['sessionToken'],\n",
    "                            region_name='us-west-2')\n",
    "rio_env = rasterio.Env(AWSSession(boto3_session),\n",
    "                  GDAL_DISABLE_READDIR_ON_OPEN='EMPTY_DIR',\n",
    "                  GDAL_HTTP_COOKIEFILE=os.path.expanduser('~/cookies.txt'),\n",
    "                  GDAL_HTTP_COOKIEJAR=os.path.expanduser('~/cookies.txt'))\n",
    "rio_env.__enter__()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcda5481-8823-4e02-8535-67f490da9782",
   "metadata": {},
   "source": [
    "### Tile downloading (Run the crendentials chunk if connecting error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee37044-857c-4d4a-a41d-971561f92ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_tiles.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3577d9aa-cf45-4cce-b0f6-df61dd51215d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tile_download(table, from_csv = True):\n",
    "    \"\"\"\n",
    "        Downloading tiles by reading from the metadata information gathered earlier\n",
    "\n",
    "        Args:\n",
    "            table: A pandas dataframe that generated previously\n",
    "            boto3_session: The session that set earlier when getting credentials\n",
    "            from_csv: If the tile information is from a csv, then True\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    info_list = []\n",
    "    bands = [\"B02\",\"B03\",\"B04\",\"B8A\",\"B11\",\"B12\",\"Fmask\"]\n",
    "    accept_tiles = np.unique(table.tile_id)\n",
    "    for tile in tqdm(accept_tiles):\n",
    "        temp_creds_req = get_temp_creds()\n",
    "\n",
    "        boto3_session = boto3.Session(aws_access_key_id=temp_creds_req['accessKeyId'], \n",
    "                                    aws_secret_access_key=temp_creds_req['secretAccessKey'],\n",
    "                                    aws_session_token=temp_creds_req['sessionToken'],\n",
    "                                    region_name='us-west-2')\n",
    "        temp_tb = table[table.tile_id == tile]\n",
    "        for i in range(3):\n",
    "            if from_csv:\n",
    "                bands_dict = json.loads(temp_tb.iloc[i].s3_links.replace(\"'\", '\"'))\n",
    "            else:\n",
    "                bands_dict = temp_tb.iloc[i].s3_links\n",
    "            for band in bands:\n",
    "                temp_key = bands_dict[band].replace(\"s3:/\", \"\")\n",
    "                temp_sav_path = f\"/data/tiles/{bands_dict[band].split('/')[2]}/{bands_dict[band].split('/')[3]}\"\n",
    "                os.makedirs(f\"/data/tiles/{bands_dict[band].split('/')[2]}\", exist_ok=True)\n",
    "                if not Path(temp_sav_path).is_file():\n",
    "                    boto3_session.resource('s3').Bucket('lp-prod-protected').download_file(Key = temp_key, Filename = temp_sav_path)\n",
    "            temp_dict = {\"tile\":tile, \"timestep\":i, \"date\":temp_tb.iloc[i].date, \"save_path\":f\"/data/tiles/{bands_dict[band].split('/')[2]}/\", \"filename\":bands_dict[\"B02\"].split('/')[3].replace(\".B02.tif\",\"\")}\n",
    "            info_list.append(temp_dict)\n",
    "    return pd.DataFrame(info_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e5a786-2fc1-48de-9931-b2cbb7ccef67",
   "metadata": {},
   "outputs": [],
   "source": [
    "track_df = tile_download(selected_tiles, from_csv=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57d8a34d-61e6-42ca-a71e-7e85a2fd67d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "track_df.to_csv(req_path + \"track_df.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "468880b3-37e1-41c4-98fd-70944d92e2bd",
   "metadata": {},
   "source": [
    "### Using the CDL tif to reproject each HLS scene to CDL projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f8fa877-023b-47c0-85cd-eddb65e222d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "track_df = pd.read_csv(req_path + \"track_df.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34938d1a-1c6e-4aec-aae7-7835583c478c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def point_transform(coor, src_crs, target_crs=5070):\n",
    "    proj = pyproj.Transformer.from_crs(src_crs, target_crs, always_xy=True)\n",
    "    projected_coor = proj.transform(coor[0], coor[1])\n",
    "    return [projected_coor[0], projected_coor[1]]\n",
    "\n",
    "def find_nearest(array, value):\n",
    "    idx = (np.abs(array - value)).argmin()\n",
    "    return array[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a3f20ce-bffe-4fb3-bb29-4a1c90c99820",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reproject_hls(tile_path,\n",
    "                  cdl_ds,\n",
    "                  target_crs =\"EPSG:5070\", \n",
    "                  remove_original = True, \n",
    "                  resampling_method = Resampling.bilinear):\n",
    "    \n",
    "    \"\"\"\n",
    "    This function receives the path to a specific HLS tile and reproject it to the targeting crs_ds.\n",
    "    The option of removing the raw HLS tile is provided\n",
    "    \n",
    "    Assumptions:\n",
    "    - tile_path is a full path that end with .tif\n",
    "    - cdl_ds is a rioxarray dataset that is opened with `cache=False` setting.\n",
    "    \n",
    "    \n",
    "    Inputs:\n",
    "    - tile_path: The full path to a specific HLS tile\n",
    "    - target_crs: The crs that you wish to reproject the tile to, default is EPSG 4326\n",
    "    - remove_original: The option to remove raw HLS tile after reprojecting, default is True\n",
    "    - resampling_method: The method that rioxarray use to reproject, default is bilinear\n",
    "    \"\"\"\n",
    "\n",
    "    xds = rioxarray.open_rasterio(tile_path)\n",
    "    half_scene_len = np.abs(np.round((xds.x.max().data - xds.x.min().data) / 2))\n",
    "    coor_min = point_transform([xds.x.min().data - half_scene_len, xds.y.min().data - half_scene_len], xds.rio.crs)\n",
    "    coor_max = point_transform([xds.x.max().data + half_scene_len, xds.y.max().data + half_scene_len], xds.rio.crs)\n",
    "    \n",
    "    x0 = find_nearest(cdl_ds.x.data, coor_min[0])\n",
    "    y0 = find_nearest(cdl_ds.y.data, coor_min[1])\n",
    "    x1 = find_nearest(cdl_ds.x.data, coor_max[0])\n",
    "    y1 = find_nearest(cdl_ds.y.data, coor_max[1])\n",
    "    \n",
    "    cdl_for_reprojection = cdl_ds.rio.slice_xy(x0, y0, x1, y1)\n",
    "    \n",
    "    xds_new = xds.rio.reproject_match(cdl_for_reprojection, resampling = resampling_method)\n",
    "\n",
    "    if remove_original:\n",
    "        if Path(tile_path).is_file():\n",
    "            os.remove(tile_path)\n",
    "        xds_new.rio.to_raster(raster_path = tile_path.replace(\".tif\", \".reproject.tif\"))\n",
    "    else:\n",
    "        xds_new.rio.to_raster(raster_path = tile_path.replace(\".tif\", \".reproject.tif\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b9d0026-b5a8-4766-aa5c-b58a9194b3c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a quality control to ensure there are three scenes for each tile.\n",
    "failed_tiles = []\n",
    "for tile in list(track_df.tile.unique()):\n",
    "    if len(track_df[track_df.tile == tile]) != 3:\n",
    "        failed_tiles.append(tile)\n",
    "if len(failed_tiles) == 0:\n",
    "    print(\"All tiles passed the quality test!\")\n",
    "else:\n",
    "    print(f\"Tile {failed_tiles} does not pass the quality test.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee2ae184-b9dc-4b90-bf50-32e291799aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "track_df[\"cdl_file\"] = cdl_file\n",
    "track_df.loc[:, \"bands\"] = '[\"B02\",\"B03\",\"B04\",\"B8A\",\"B11\",\"B12\",\"Fmask\"]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "524e4a00-1773-4615-b844-f60d4ba52eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "track_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4530d1d2-a04d-431e-8b89-51651163537d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hls_process(kwargs):\n",
    "\n",
    "    remove_original = True\n",
    "    \n",
    "    save_path = kwargs[\"save_path\"]\n",
    "    filename= kwargs[\"filename\"]\n",
    "    bands = json.loads(kwargs[\"bands\"])\n",
    "    cdl_file = kwargs[\"cdl_file\"]\n",
    "    \n",
    "    cdl_ds = rioxarray.open_rasterio(cdl_file, cache=False)\n",
    "    \n",
    "    for band in bands:\n",
    "        tile_path = f\"{save_path}{filename}.{band}.tif\"\n",
    "        if Path(tile_path).is_file():\n",
    "            if band == \"Fmask\":\n",
    "                reproject_hls(tile_path, cdl_ds, remove_original, resampling_method = Resampling.nearest)\n",
    "            else :\n",
    "                reproject_hls(tile_path, cdl_ds, remove_original) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "283baa4e-5df7-47ad-9026-6efc7bed6145",
   "metadata": {},
   "outputs": [],
   "source": [
    "with mp.Pool(processes=5) as pool:\n",
    "    pool.map(hls_process, track_df.to_dict('records'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fba840c-33c6-4fb3-917f-538411ba38da",
   "metadata": {},
   "source": [
    "### Chipping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28119f07-521e-4356-a803-6c5e894ebe16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting all saved dataframes and json\n",
    "chip_df = pd.read_csv(req_path + \"chip_df.csv\")\n",
    "with open(\"/data/chip_ids.json\", 'r') as f:\n",
    "    chip_ids = json.load(f)\n",
    "track_df = pd.read_csv(req_path + \"track_df.csv\")\n",
    "with open(chip_file, \"r\") as file:\n",
    "    chips = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38d86289-e212-41b3-9381-a30ae0ebb171",
   "metadata": {},
   "outputs": [],
   "source": [
    "tiles_to_chip = track_df.tile.unique().tolist()\n",
    "with open(chipping_json, \"r\") as file_chip:\n",
    "    chipping_js = json.load(file_chip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11e4d2c0-4f6c-4873-a611-49ae4c2eba52",
   "metadata": {},
   "outputs": [],
   "source": [
    "## set up CDL reclass\n",
    "cdl_class_df = pd.read_csv(cdl_reclass_csv)\n",
    "crop_dict = dict(zip(cdl_class_df.old_class_value, cdl_class_df.new_class_value))\n",
    "\n",
    "def crop_multi(x):\n",
    "    return(crop_dict[x])\n",
    "\n",
    "c_multi = np.vectorize(crop_multi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c4dbdf-ece0-45f8-803a-cfe36abb41c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_qa(qa_path, shape,  valid_qa = [0, 4, 32, 36, 64, 68, 96, 100, 128, 132, 160, 164, 192, 196, 224, 228]):\n",
    "    \n",
    "    \"\"\"\n",
    "    This function receives a path to a qa file, and a geometry. It clips the QA file to the geometry. \n",
    "    It returns the number of valid QA pixels in the geometry, and the clipped values.\n",
    "    \n",
    "    Assumptions: The valid_qa values are taken from Ben Mack's post:\n",
    "    https://benmack.github.io/nasa_hls/build/html/tutorials/Working_with_HLS_datasets_and_nasa_hls.html\n",
    "    \n",
    "    Inputs:\n",
    "    - qa_path: full path to reprojected QA tif file\n",
    "    - shape: 'geometry' property of single polygon feature read by fiona\n",
    "    - valid_qa: list of integer values that are 'valid' for QA band.\n",
    "    \n",
    "\n",
    "    \n",
    "    \"\"\"\n",
    "    with rasterio.open(qa_path) as src:\n",
    "        out_image, out_transform = rasterio.mask.mask(src, shape, crop=True)\n",
    "        vals = out_image.flatten()\n",
    "        unique, counts = np.unique(vals, return_counts=True)\n",
    "        qa_df = pd.DataFrame({\"qa_val\" : unique, \"counts\" : counts})\n",
    "        qa_df\n",
    "        qa_df[~ qa_df.qa_val.isin(valid_qa)].sort_values(['counts'], ascending = False)\n",
    "        qa_df['pct'] = (100 *qa_df['counts'])/(224.0 * 224.0)\n",
    "        \n",
    "        bad_qa = qa_df[~ qa_df.qa_val.isin(valid_qa)].sort_values(['counts'], ascending = False)\n",
    "        if len(bad_qa) > 0:\n",
    "            highest_invalid_percent = bad_qa.pct.tolist()[0]\n",
    "        else: \n",
    "            highest_invalid_percent = 0\n",
    "        # ncell = len(vals)\n",
    "        valid_count = sum(x in valid_qa for x in vals)\n",
    "        return(valid_count, highest_invalid_percent, out_image[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "354ebe46-35c7-473a-8827-7348fa2a0249",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_chip(chip_id, \n",
    "                 chip_tile,\n",
    "                 shape,\n",
    "                 track_csv,\n",
    "                 cdl_file,\n",
    "                 bands = [\"B02\", \"B03\", \"B04\", \"B8A\", \"B11\", \"B12\"]):\n",
    "    \n",
    "    \"\"\"\n",
    "    This function receives a chip id, HLS tile, chip geometry, and a list of bands to process. \n",
    "    \n",
    "    Assumptions:\n",
    "    \n",
    "    Inputs:\n",
    "    - chip_id: string of chip id, e.g. '000_001'\n",
    "    - chip_tile: string of HLS tile , e.g. '15ABC'\n",
    "    - shape: 'geometry' property of single polygon feature read by fiona\n",
    "    \n",
    "    The function writes out a multi-date TIF containing the bands for each of the three image dates for an HLS tile. \n",
    "    The function writes out a multi-date TIF containing the QA bands of each date.\n",
    "    The function writes out a chipped version of CDL. \n",
    "    The function calls check_qa(), which makes assumptions about what QA pixels are valid.\n",
    "    The function returns the number of valid QA pixels at each date, as a tuple.\n",
    "    \n",
    "    \"\"\"\n",
    "    ## get reprojected image paths\n",
    "    tile_info_df = track_csv[track_csv.tile == chip_tile]\n",
    "    \n",
    "    selected_image_folders = tile_info_df.save_path.to_list()\n",
    "\n",
    "    \n",
    "    assert len(selected_image_folders) == 3\n",
    "    \n",
    "                     \n",
    "    first_image_date = tile_info_df.iloc[0].date\n",
    "    second_image_date = tile_info_df.iloc[1].date\n",
    "    third_image_date = tile_info_df.iloc[2].date\n",
    "    \n",
    "\n",
    "    all_date_images = []\n",
    "    all_date_qa = []\n",
    "                     \n",
    "    for i in range(3):\n",
    "        for band in bands:\n",
    "            all_date_images.append(tile_info_df.iloc[i].save_path + f\"{tile_info_df.iloc[i].filename}.{band}.reproject.tif\")\n",
    "        all_date_qa.append(tile_info_df.iloc[i].save_path + f\"{tile_info_df.iloc[i].filename}.Fmask.reproject.tif\")\n",
    "        \n",
    "\n",
    "    valid_first, bad_pct_first, qa_first = check_qa(all_date_qa[0], shape)\n",
    "    valid_second, bad_pct_second, qa_second = check_qa(all_date_qa[1], shape)\n",
    "    valid_third, bad_pct_third, qa_third = check_qa(all_date_qa[2], shape)\n",
    "    \n",
    "    qa_bands = []\n",
    "    qa_bands.append(qa_first)\n",
    "    qa_bands.append(qa_second)\n",
    "    qa_bands.append(qa_third)\n",
    "    qa_bands = np.array(qa_bands).astype(np.uint8)\n",
    "    \n",
    "\n",
    "    assert len(all_date_images) == 3 * len(bands)\n",
    "    \n",
    "    out_bands = []\n",
    "    print('out_bands_loop')\n",
    "    for img in all_date_images:\n",
    "        with rasterio.open(img) as src:\n",
    "            print(img)\n",
    "            out_image, out_transform = rasterio.mask.mask(src, shape, crop=True)\n",
    "            out_meta = src.meta\n",
    "            out_bands.append(out_image[0])\n",
    "    \n",
    "    out_bands = np.array(out_bands)\n",
    "    # print(out_bands.shape)\n",
    "\n",
    "\n",
    "    out_meta.update({\"driver\": \"GTiff\",\n",
    "                     \"height\": out_bands.shape[1],\n",
    "                     \"width\": out_bands.shape[2],\n",
    "                     \"count\": out_bands.shape[0],\n",
    "                     \"transform\": out_transform})\n",
    "    \n",
    "    # get NA count for HLS\n",
    "    na_count = sum(out_bands.flatten() == -1000)\n",
    "    \n",
    "    # reclass negative HLS values to 0\n",
    "    out_bands = np.clip(out_bands, 0, None)\n",
    "    \n",
    "    # write HLS chips\n",
    "    with rasterio.open(chip_dir + str(chip_id) + \"_merged.tif\", \"w\", **out_meta) as dest:\n",
    "        dest.write(out_bands)\n",
    "      \n",
    "    \n",
    "        \n",
    "    ## write QA bands\n",
    "    out_meta.update({\"driver\": \"GTiff\",\n",
    "                     \"height\": qa_bands.shape[1],\n",
    "                     \"width\": qa_bands.shape[2],\n",
    "                     \"count\": qa_bands.shape[0],\n",
    "                     \"transform\": out_transform})\n",
    "    \n",
    "    with rasterio.open(chip_fmask_dir + str(chip_id) + \"_Fmask.tif\", \"w\", **out_meta) as dest:\n",
    "        dest.write(qa_bands)  \n",
    "\n",
    "    \n",
    "    ## clip cdl to chip\n",
    "    with rasterio.open(cdl_file) as src:\n",
    "        out_image, out_transform = rasterio.mask.mask(src, shape, crop=True)\n",
    "        out_meta = src.meta\n",
    "        colormap = src.colormap(1)\n",
    "\n",
    "    out_meta.update({\"driver\": \"GTiff\",\n",
    "                     \"height\": out_image.shape[1],\n",
    "                     \"width\": out_image.shape[2],\n",
    "                     \"transform\": out_transform})\n",
    "    \n",
    "    # write multiclass reclassed CDL chips\n",
    "    out_image_multi = c_multi(out_image).astype(np.uint8)\n",
    "    with rasterio.open(chip_dir + str(chip_id) + \".mask.tif\", \"w\", **out_meta) as dest:\n",
    "        dest.write(out_image_multi)\n",
    "        dest.write_colormap(1, colormap)\n",
    "\n",
    "                     \n",
    "    return(valid_first,\n",
    "           valid_second,\n",
    "           valid_third, \n",
    "           bad_pct_first,\n",
    "           bad_pct_second,\n",
    "           bad_pct_third,\n",
    "           na_count,\n",
    "           first_image_date,\n",
    "           second_image_date,\n",
    "           third_image_date)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0088c95c-885d-4846-b357-b298f8ba4641",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## process chips\n",
    "failed_tiles = []\n",
    "\n",
    "for tile in tiles_to_chip:\n",
    "    print(tile)\n",
    "    chips_to_process = chip_df[chip_df.tile == tile[1:]].reset_index(drop = True)\n",
    "    for k in range(len(chips_to_process)):\n",
    "        current_id = chips_to_process.chip_id[k]\n",
    "        chip_tile = chips_to_process.tile[k]\n",
    "        chip_index = chip_ids.index(current_id)\n",
    "\n",
    "        chip_feature = chipping_js['features'][chip_index]\n",
    "\n",
    "        shape = [chip_feature['geometry']]\n",
    "        full_tile_name = \"T\" + chip_tile\n",
    "        try:\n",
    "            valid_first, valid_second, valid_third, bad_pct_first, bad_pct_second, bad_pct_third, na_count, first_image_date, second_image_date, third_image_date = process_chip(current_id, full_tile_name, shape, track_df, cdl_file)\n",
    "        except:\n",
    "            failed_tiles.append(tile)\n",
    "            break\n",
    "\n",
    "        chip_df_index = chip_df.index[chip_df['chip_id'] == current_id].tolist()[0]\n",
    "        chip_df.at[chip_df_index, 'valid_first'] = valid_first\n",
    "        chip_df.at[chip_df_index, 'valid_second'] = valid_second\n",
    "        chip_df.at[chip_df_index, 'valid_third'] = valid_third\n",
    "        chip_df.at[chip_df_index, 'bad_pct_first'] = bad_pct_first\n",
    "        chip_df.at[chip_df_index, 'bad_pct_second'] = bad_pct_second\n",
    "        chip_df.at[chip_df_index, 'bad_pct_third'] = bad_pct_third\n",
    "        chip_df.at[chip_df_index, 'first_image_date'] = first_image_date\n",
    "        chip_df.at[chip_df_index, 'second_image_date'] = second_image_date\n",
    "        chip_df.at[chip_df_index, 'third_image_date'] = third_image_date\n",
    "        chip_df['bad_pct_max'] = chip_df[['bad_pct_first', 'bad_pct_second', 'bad_pct_third']].max(axis=1)\n",
    "        chip_df.at[chip_df_index, 'na_count'] = na_count\n",
    "chip_df.to_csv(chip_csv, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c193dd-4761-4c26-b440-c214fbf79a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "chip_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdfb667c-d9e4-446a-9149-3b7990e60753",
   "metadata": {},
   "source": [
    "### Filtering Chips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8960db6e-d89f-4da7-a878-ff05824f4bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_tiles = pd.read_csv(req_path + \"selected_tiles.csv\")\n",
    "tiles_to_filter = selected_tiles.tile_id.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0029c420-5b71-4600-9838-c0b878835fe4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "chip_df = pd.read_csv(chip_csv)\n",
    "\n",
    "for tile in tiles_to_filter:\n",
    "    print(tile)\n",
    "    filtered_chips = chip_df[(chip_df.tile == tile[1:]) & (chip_df.bad_pct_max < 5) & (chip_df.na_count == 0)].chip_id.tolist()\n",
    "    for chip_id in filtered_chips:\n",
    "        chip_files = glob('/data/chips/*' + chip_id + '*')\n",
    "        for file in chip_files:\n",
    "            name = file.split('/')[-1]\n",
    "            shutil.copyfile(file, chip_dir_filt + name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "220ddeb4-abb5-4c2f-a7c4-af082898950f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
