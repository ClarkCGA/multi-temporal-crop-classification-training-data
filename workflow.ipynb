{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a5e493cc-09bc-4d70-8316-c8aa4021de9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import rioxarray \n",
    "import xarray\n",
    "import rasterio\n",
    "import nasa_hls\n",
    "import os\n",
    "import geopandas\n",
    "import urllib.request as urlreq\n",
    "import pandas as pd\n",
    "import fiona\n",
    "import numpy as np\n",
    "import json\n",
    "import shutil\n",
    "import datetime\n",
    "from glob import glob\n",
    "from rasterio.enums import Resampling\n",
    "from rasterio import Affine\n",
    "from rasterio.crs import CRS\n",
    "import matplotlib.pyplot as plt\n",
    "from subprocess import Popen, PIPE\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a17b27be-7dfe-43b1-a775-efe885235127",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### START OPTIONS #####\n",
    "yoi = [2021]\n",
    "#toi = ['15STT']\n",
    "cloud_thres = 5\n",
    "valid_months = [3,4,5,6,7,8,9]\n",
    "root_path = \"/data/\"\n",
    "\n",
    "## file paths\n",
    "spath = root_path + f\"CDL_HLS_dataframe{yoi[0]}\"\n",
    "image_index_file = root_path + f\"image_index{yoi[0]}\"\n",
    "chip_file =  root_path + \"chip_bbox.geojson\"\n",
    "chip_csv = root_path + \"chip_tracker.csv\"\n",
    "kml_file = root_path + 'sentinel_tile_grid.kml'\n",
    "cdl_reclass_csv = root_path + \"cdl_freq.csv\"\n",
    "tile_tracker_csv = root_path + \"tile_tracker.csv\"\n",
    "\n",
    "## folder paths\n",
    "hdf_dir = root_path + \"hdf/\"\n",
    "chip_dir = root_path + 'chips/'\n",
    "tif_dir = root_path + 'tif/'\n",
    "chip_dir_binary = root_path + 'chips_binary/'\n",
    "chip_dir_multi = root_path + 'chips_multi/'\n",
    "\n",
    "chip_dir_filt = root_path + 'chips_filtered/'\n",
    "chip_dir_binary_filt = root_path + 'chips_binary_filtered/'\n",
    "chip_dir_multi_filt = root_path + 'chips_multi_filtered/'\n",
    "\n",
    "chip_qa_dir = root_path + 'chips_qa/'\n",
    "\n",
    "#####  END OPTIONS  #####"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b61d21b7-d5a1-468f-ae9b-36a6aaa7f0f1",
   "metadata": {},
   "source": [
    "make folders if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8b4eee91-56f0-43f7-98a9-75a9c10aaf1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pass\n",
      "pass\n",
      "pass\n",
      "pass\n",
      "pass\n",
      "pass\n",
      "pass\n",
      "pass\n",
      "pass\n"
     ]
    }
   ],
   "source": [
    "dirs_to_make = [hdf_dir, chip_dir, chip_dir_binary, chip_qa_dir, chip_dir_binary, chip_dir_multi, chip_dir_binary_filt, chip_dir_filt, chip_dir_multi_filt]\n",
    "for folder in dirs_to_make:\n",
    "    try:\n",
    "        os.makedirs(folder)\n",
    "    except FileExistsError:\n",
    "        # directory already exists\n",
    "        print('pass')\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f38a44b2-43b7-40f6-b03d-67a5a8a3035b",
   "metadata": {},
   "source": [
    "0 determine HLS tiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fa9b6751-52c3-4fe8-b38e-1721dd7db78f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/cdl_training_data/data/chip_bbox.geojson\", \"r\") as file:\n",
    "    chips = json.load(file)\n",
    "    \n",
    "chip_ids = []\n",
    "chip_x = []\n",
    "chip_y = []\n",
    "for item in chips['features']:\n",
    "    #print(item)\n",
    "    chip_ids.append(item['properties']['id'])\n",
    "    chip_x.append(item['properties']['center'][0])\n",
    "    chip_y.append(item['properties']['center'][1])\n",
    "\n",
    "\n",
    "#chip_ids = a.fea\n",
    "#print(a['features'][0]['properties']['center'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ca25ef44-9574-43e0-84bb-8349699bfb77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Description</th>\n",
       "      <th>geometry</th>\n",
       "      <th>minx</th>\n",
       "      <th>miny</th>\n",
       "      <th>maxx</th>\n",
       "      <th>maxy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>01CCV</td>\n",
       "      <td>TILE PROPERTIES&lt;br&gt;&lt;table border=0 cellpadding...</td>\n",
       "      <td>GEOMETRYCOLLECTION Z (POLYGON Z ((180.00000 -7...</td>\n",
       "      <td>-180.0</td>\n",
       "      <td>-73.064633</td>\n",
       "      <td>180.0</td>\n",
       "      <td>-72.012478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>01CDH</td>\n",
       "      <td>TILE PROPERTIES&lt;br&gt;&lt;table border=0 cellpadding...</td>\n",
       "      <td>GEOMETRYCOLLECTION Z (POLYGON Z ((180.00000 -8...</td>\n",
       "      <td>-180.0</td>\n",
       "      <td>-83.835334</td>\n",
       "      <td>180.0</td>\n",
       "      <td>-82.796720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>01CDJ</td>\n",
       "      <td>TILE PROPERTIES&lt;br&gt;&lt;table border=0 cellpadding...</td>\n",
       "      <td>GEOMETRYCOLLECTION Z (POLYGON Z ((180.00000 -8...</td>\n",
       "      <td>-180.0</td>\n",
       "      <td>-82.939452</td>\n",
       "      <td>180.0</td>\n",
       "      <td>-81.906947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>01CDK</td>\n",
       "      <td>TILE PROPERTIES&lt;br&gt;&lt;table border=0 cellpadding...</td>\n",
       "      <td>GEOMETRYCOLLECTION Z (POLYGON Z ((180.00000 -8...</td>\n",
       "      <td>-180.0</td>\n",
       "      <td>-82.044055</td>\n",
       "      <td>180.0</td>\n",
       "      <td>-81.016439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>01CDL</td>\n",
       "      <td>TILE PROPERTIES&lt;br&gt;&lt;table border=0 cellpadding...</td>\n",
       "      <td>GEOMETRYCOLLECTION Z (POLYGON Z ((180.00000 -8...</td>\n",
       "      <td>-180.0</td>\n",
       "      <td>-81.148070</td>\n",
       "      <td>180.0</td>\n",
       "      <td>-80.124456</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Name                                        Description  \\\n",
       "0  01CCV  TILE PROPERTIES<br><table border=0 cellpadding...   \n",
       "1  01CDH  TILE PROPERTIES<br><table border=0 cellpadding...   \n",
       "2  01CDJ  TILE PROPERTIES<br><table border=0 cellpadding...   \n",
       "3  01CDK  TILE PROPERTIES<br><table border=0 cellpadding...   \n",
       "4  01CDL  TILE PROPERTIES<br><table border=0 cellpadding...   \n",
       "\n",
       "                                            geometry   minx       miny   maxx  \\\n",
       "0  GEOMETRYCOLLECTION Z (POLYGON Z ((180.00000 -7... -180.0 -73.064633  180.0   \n",
       "1  GEOMETRYCOLLECTION Z (POLYGON Z ((180.00000 -8... -180.0 -83.835334  180.0   \n",
       "2  GEOMETRYCOLLECTION Z (POLYGON Z ((180.00000 -8... -180.0 -82.939452  180.0   \n",
       "3  GEOMETRYCOLLECTION Z (POLYGON Z ((180.00000 -8... -180.0 -82.044055  180.0   \n",
       "4  GEOMETRYCOLLECTION Z (POLYGON Z ((180.00000 -8... -180.0 -81.148070  180.0   \n",
       "\n",
       "        maxy  \n",
       "0 -72.012478  \n",
       "1 -82.796720  \n",
       "2 -81.906947  \n",
       "3 -81.016439  \n",
       "4 -80.124456  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the HLS tiles and place there coordinates into a numpy array for processing later\n",
    "\n",
    "fiona.drvsupport.supported_drivers['KML'] = 'rw'\n",
    "tile_src = geopandas.read_file(kml_file, driver='KML')\n",
    "tile_name = []\n",
    "tile_x = []\n",
    "tile_y = []\n",
    "for tile_ind in range(tile_src.shape[0]):\n",
    "    tile_name.append(tile_src.iloc[tile_ind].Name)\n",
    "    tile_x.append(tile_src.iloc[tile_ind].geometry.centroid.x)\n",
    "    tile_y.append(tile_src.iloc[tile_ind].geometry.centroid.y)\n",
    "tile_name = np.array(tile_name)\n",
    "tile_x = np.array(tile_x)\n",
    "tile_y = np.array(tile_y)\n",
    "tile_src = pd.concat([tile_src, tile_src.bounds], axis = 1)\n",
    "#del tile_src\n",
    "tile_src.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eec16f55-ba54-49a9-816e-bb7dfc740b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_tile(x,y):\n",
    "# Identify closest tile\n",
    "    s = (tile_x - x)**2+(tile_y - y)**2\n",
    "    tname = tile_name[np.argmin(s)]\n",
    "    return(tname)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f78d885-c7ce-41e7-94c7-374d51cb4003",
   "metadata": {},
   "source": [
    "initialize chip tracker csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1cbe8e0-e2b7-4cc0-b970-2bb85eb41948",
   "metadata": {},
   "outputs": [],
   "source": [
    "chip_df = pd.DataFrame({\"chip_id\" : chip_ids, \"chip_x\" : chip_x, \"chip_y\" : chip_y})\n",
    "chip_df['tile'] = chip_df.apply(lambda row : find_tile(row['chip_x'], row['chip_y']), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "416d62fa-c766-417f-ac34-aac72600b20f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## write to csv\n",
    "check_file = glob(chip_csv)\n",
    "if len(check_file) == 0:\n",
    "    chip_df.to_csv(chip_csv, index=False)\n",
    "else:\n",
    "    print('file exists')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c938ca81-9f17-494e-97c7-4d55bd66a7f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tiles = chip_df.tile.unique().tolist()\n",
    "tiles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68e306e5-a77b-4d54-b970-20f1985f07a8",
   "metadata": {},
   "source": [
    "0a. manually chack and remove \"bad\" tiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d437a0bd-bdd3-4fae-ad06-01a11279bb63",
   "metadata": {},
   "outputs": [],
   "source": [
    "chip_df[chip_df.tile == '01SBU'].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5081f11f-f7fd-476c-8d40-5eb1ccd1bed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tiles.remove('01SBU')\n",
    "tiles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43ecb92b-8468-478f-b7a4-5ca4cf347cc8",
   "metadata": {},
   "source": [
    "0b. Make tile tracker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78409c49-71c7-40cb-b091-6760c0da04d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tile_tracker = pd.DataFrame({\"tile\":tiles})\n",
    "tile_tracker['exclude'] = False\n",
    "tile_tracker['hdf_download'] = False\n",
    "tile_tracker['tif_convert'] = False\n",
    "tile_tracker['tif_reproject'] = False\n",
    "tile_tracker['chip'] = False\n",
    "tile_tracker['filter_chips'] = False\n",
    "#tile_tracker.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9350fd8-40a2-4e71-b18d-9cc746212575",
   "metadata": {},
   "outputs": [],
   "source": [
    "## update tracker\n",
    "tiles_already_downloaded = glob(hdf_dir + '*')\n",
    "tiles_already_downloaded = set([i[19:24] for i in tiles_already_downloaded])\n",
    "tiles_already_downloaded\n",
    "tile_tracker.loc[tile_tracker.tile.isin(tiles_already_downloaded) , 'hdf_download'] = True\n",
    "\n",
    "tiles_already_converted = glob(tif_dir + '*')\n",
    "tiles_already_converted = set([i[19:24] for i in tiles_already_converted])\n",
    "tiles_already_converted\n",
    "tile_tracker.loc[tile_tracker.tile.isin(tiles_already_converted) , 'tif_convert'] = True\n",
    "\n",
    "chips_already_chipped = glob(chip_dir + '*')\n",
    "chips_already_chipped = set([i[17:24] for i in chips_already_chipped])\n",
    "#print(chips_already_chipped)\n",
    "tiles_already_chipped = chip_df[chip_df.chip_id.isin(chips_already_chipped)].tile.unique()\n",
    "#print(tiles_already_chipped)\n",
    "tile_tracker.loc[tile_tracker.tile.isin(tiles_already_chipped) , 'tif_reproject'] = True\n",
    "tile_tracker.loc[tile_tracker.tile.isin(tiles_already_chipped) , 'chip'] = True\n",
    "                                        \n",
    "chips_already_filtered = glob(chip_dir_filt + '*')\n",
    "chips_already_filtered = set([i[26:33] for i in chips_already_filtered])\n",
    "tiles_already_filtered = chip_df[chip_df.chip_id.isin(chips_already_filtered)].tile.unique()\n",
    "tile_tracker.loc[tile_tracker.tile.isin(tiles_already_filtered), 'filter_chips'] = True\n",
    "\n",
    "tile_tracker.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fad3d6c-74a0-499a-8f37-a99b1cce8480",
   "metadata": {},
   "outputs": [],
   "source": [
    "## write to csv\n",
    "check_file = glob(tile_tracker_csv)\n",
    "if len(check_file) == 0:\n",
    "    tile_tracker.to_csv(tile_tracker_csv, index=False)\n",
    "else:\n",
    "    print('file exists')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad736731-d5a5-4f32-b2fb-8580427c76f3",
   "metadata": {},
   "source": [
    "1. query and download hdf files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f4fff5c-4768-4dbc-9ea8-590869e7aa53",
   "metadata": {},
   "outputs": [],
   "source": [
    "toi = tile_tracker[(tile_tracker.exclude == False) & (tile_tracker.hdf_download == False)].tile.unique()\n",
    "toi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38cea45c-e07f-4813-91f6-47401f1d2277",
   "metadata": {},
   "source": [
    "1a. get URLs of hdf to download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e8c67cc-5e3f-4c79-9b97-5d32b9cbab41",
   "metadata": {},
   "outputs": [],
   "source": [
    "HLSdf = nasa_hls.get_available_datasets(\n",
    "        years = yoi,\n",
    "        products = [\"S30\"],\n",
    "        tiles = toi,\n",
    "        return_list = False)\n",
    "\n",
    "#HLSdf.to_csv(spath, mode='w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede76c7d-78ef-4c89-ae9b-6853fe45e209",
   "metadata": {},
   "outputs": [],
   "source": [
    "HLSdf['month'] = pd.DatetimeIndex(HLSdf['date']).month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8fc7289-52ae-474c-b8d5-e1efc13b0965",
   "metadata": {},
   "outputs": [],
   "source": [
    "## filter by month\n",
    "HLSdf = HLSdf[HLSdf.month.isin(valid_months)].reset_index(drop = True)\n",
    "HLSdf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c21e9b4-5602-46af-85bd-da878fb8cf32",
   "metadata": {},
   "outputs": [],
   "source": [
    "## download hdf\n",
    "for k in range(len(HLSdf)):\n",
    "    url = HLSdf.at[k, \"url\"]\n",
    "    local_name = url.split('/')[-1].replace(\"\\n\", \"\").replace('.hdf', '')\n",
    "    HLSdf.at[k, \"image_id\"] = local_name\n",
    "    try:\n",
    "        urlreq.urlretrieve(url, filename = hdf_dir+local_name + '.hdf')\n",
    "    except:\n",
    "        print(local_name + \" failed\")\n",
    "        continue\n",
    "\n",
    "tile_tracker = pd.read_csv(tile_tracker_csv)\n",
    "tile_tracker.loc[tile_tracker.tile.isin(toi) , 'hdf_download'] = True\n",
    "tile_tracker.to_csv(tile_tracker_csv, index=False)\n",
    "#ct = datetime.datetime.now()\n",
    "#HLSdf.to_csv(spath + \"_\" + str(ct) + \".csv\", mode='w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "266c403d-4845-487e-bddb-de50eab83548",
   "metadata": {},
   "outputs": [],
   "source": [
    "tile_tracker.head(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58b07887-edf0-4076-bd16-a8173cc098ca",
   "metadata": {},
   "source": [
    "2. extract hdf metadata, filter to 3 scenes per tile, convert to tif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0afa7935-6c59-437b-b7db-a126599ac27b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metadata_from_hdf_mine(src, fields=[\"cloud_cover\", \"spatial_coverage\"]):\n",
    "    \"\"\"Get metadata from a nasa-hls hdf file. See HLS user guide for valid fields.\n",
    "    \n",
    "    HLS User Guide - see Section 6.6: \n",
    "    \n",
    "    https://hls.gsfc.nasa.gov/wp-content/uploads/2019/01/HLS.v1.4.UserGuide_draft_ver3.1.pdf\n",
    "    \"\"\"\n",
    "    band=\"QA\"\n",
    "    cmd = f'gdalinfo HDF4_EOS:EOS_GRID:\"{src}\":Grid:{band}'\n",
    "#    print(cmd)\n",
    "    p = Popen(cmd, stdout=PIPE, shell=True)\n",
    "    output, err = p.communicate()\n",
    "    output = str(output)[2:-1].replace(\"\\\\n\", \"\\n\")\n",
    "    rc = p.returncode\n",
    "    metadata = {}\n",
    "    for line in output.split(\"\\n\"):\n",
    "        for field in fields:\n",
    "            if field in line:\n",
    "                metadata[field] = line.split(\"=\")[1].strip()\n",
    "                try:\n",
    "                    metadata[field] = float(metadata[field])\n",
    "                except:\n",
    "                    pass\n",
    "    for field in fields:\n",
    "        if field not in metadata.keys():\n",
    "            warnings.warn(f\"Could not find metadata for field '{field}'.\")\n",
    "    return metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35173684-e5bc-42cf-8ba9-47623fa093c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_index = pd.DataFrame(columns = ['image_id', 'tile', 'date', 'month', 'cloud_coverage', 'spatial_coverage'])\n",
    "\n",
    "candidate_hdf = sorted(glob(hdf_dir + '*.hdf'))\n",
    "\n",
    "for img in candidate_hdf:\n",
    "   # print(img)\n",
    "    local_name = img.split('/')[-1]\n",
    "    try:\n",
    "      #  print(hdf_dir+local_name)\n",
    "        md = get_metadata_from_hdf_mine(hdf_dir+local_name)\n",
    "    except:\n",
    "        print(img + ' skipped')\n",
    "        continue\n",
    "   # print(md)\n",
    "    cloud_cover = int(md['cloud_cover'])\n",
    "    spatial_coverage = int(md['spatial_coverage'])\n",
    "    image_id = local_name.replace('.hdf', '')\n",
    "    tname = local_name.split('.')[2][1:]\n",
    "    date = local_name.split('.')[3]\n",
    "    image_date_string = image_id.split('.')[3]\n",
    "    image_date = pd.to_datetime(image_date_string, format=\"%Y%j\").date()\n",
    "    image_month = image_date.month\n",
    "    \n",
    "    new_row = pd.DataFrame({'image_id':  [image_id],\n",
    "               'tile': [tname],\n",
    "               'date': [image_date],\n",
    "               'month': [image_month],\n",
    "               'cloud_coverage': [cloud_cover],\n",
    "               'spatial_coverage': [spatial_coverage]})\n",
    "    image_index = pd.concat([image_index, new_row], ignore_index = True)\n",
    "\n",
    "ct = datetime.datetime.now()\n",
    "image_index.to_csv(image_index_file + \"_\" + str(ct) + '.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6899c71-6d74-4c41-8901-b31eff2990fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#image_index.sort_values(['cloud_coverage']).head(300)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a47e0aad-c5d4-4c91-865b-de028ecd1d20",
   "metadata": {},
   "source": [
    "Select 3 best images (need to loop this over tiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb4a40f7-fedc-4dda-b90e-af5ffbd88ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tile_tracker = pd.read_csv(tile_tracker_csv)\n",
    "#tiles_already_converted = set([i[19:24] for i in tiles_already_converted])\n",
    "tiles_to_process = tile_tracker[(tile_tracker.exclude == False) & (tile_tracker.tif_convert == False) & (tile_tracker.hdf_download == True)].tile.unique()\n",
    "tiles_to_process\n",
    "#tile_tracker.to_csv(tile_tracker_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "381de00f-9871-4fb9-a61b-712ea5a81c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_hdf_to_cog(scene_id, product = \"S30\"):\n",
    "    \n",
    "    \"\"\"\n",
    "    This function receives the scene_id of an HLS scene (in a format similar to \"HLS.S30.T14RNS.2020005.v1.4\"\n",
    "    and converts the scene from HDF format to COG. \n",
    "    \n",
    "    Assumptions:\n",
    "    - The corresponding HDF file for the scene is located at `/data/hdf/scene_id.hdf`\n",
    "    - The output will be written to `/data/tif/scene_id/*.tif` and contains all the bands. \n",
    "    \n",
    "    Inputs:\n",
    "    - scene_id: The scene ID of the HLS scene\n",
    "    - product: the HLS product ID. Default is S30, but it can be S30, L30, S30_ANGLES, L30_ANGLES\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    import os\n",
    "    cmd = f\"python3 /hls-hdf_to_cog/hls_hdf_to_cog/hls_hdf_to_cog.py --product {product} /data/hdf/{scene_id}.hdf --output-dir /data/tif/{scene_id}/\"\n",
    "    os.system(cmd)\n",
    "    image_folder = '/data/tif/' + scene_id + '/'\n",
    "    tif_count = len(glob(image_folder + '*.tif'))\n",
    "    if(tif_count == 14):\n",
    "        return(True)\n",
    "    else:\n",
    "        shutil.rmtree(image_folder)\n",
    "        return(False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00d66cb1-f0b3-4cfb-bb73-1aa2f46ea386",
   "metadata": {},
   "source": [
    "convert selected hdf to cog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b812826-7328-4a05-a005-8e3f6e2f6215",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_first_date(cand_images):\n",
    "    \"\"\"\n",
    "    Converts first date image from data frame. \n",
    "    If conversion fails, the image is removed and the next \"first\" image is tried.\n",
    "    Returns the converted image row, the image id, and the data frame with any failed images removed.\n",
    "    \"\"\"\n",
    "    process_first = False\n",
    "    while process_first == False:\n",
    "        first_image = cand_images.head(1)\n",
    "        first_image_id = (first_image.image_id.tolist())[0]\n",
    "        print(first_image_id)\n",
    "        process_first = convert_hdf_to_cog(first_image_id)\n",
    "        print(process_first)\n",
    "        if(process_first == False):\n",
    "            cand_images = cand_images[cand_images.image_id != first_image_id]\n",
    "    return(first_image, first_image_id, cand_images)\n",
    "\n",
    "def convert_last_date(cand_images):\n",
    "    \"\"\"\n",
    "    Converts last date image from data frame. \n",
    "    If conversion fails, the image is removed and the next \"last\" image is tried.\n",
    "    Returns the converted image row, the image id, and the data frame with any failed images removed.\n",
    "    \"\"\"\n",
    "    process_last = False\n",
    "    while process_last == False:\n",
    "        last_image = cand_images.tail(1)\n",
    "        last_image_id = (last_image.image_id.tolist())[0]\n",
    "        print(last_image_id)\n",
    "        process_last = convert_hdf_to_cog(last_image_id)\n",
    "        print(process_last)\n",
    "        if(process_last == False):\n",
    "            cand_images = cand_images[cand_images.image_id != last_image_id]\n",
    "    return(last_image, last_image_id, cand_images)\n",
    "\n",
    "def convert_middle_date(cand_images):\n",
    "    \"\"\"\n",
    "    Converts middle date image from data frame. \n",
    "    If conversion fails, the image is removed and the next \"middle\" image is tried.\n",
    "    Returns the converted image row, the image id, and the data frame with any failed images removed.\n",
    "    \"\"\"\n",
    "    process_middle = False\n",
    "    cand_image_count = len(cand_images)\n",
    "    while process_middle == False:\n",
    "        middle_image = cand_images.head((cand_image_count // 2)+1).tail(1)\n",
    "        middle_image_id = (middle_image.image_id.tolist())[0]\n",
    "        print(middle_image_id)\n",
    "        process_middle = convert_hdf_to_cog(middle_image_id)\n",
    "        print(process_middle)\n",
    "        if(process_middle == False):\n",
    "            cand_images = cand_images[cand_images.image_id != middle_image_id]\n",
    "    return(middle_image, middle_image_id, cand_images)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b5bf9ee-c425-4d94-bb33-1c00cd6df882",
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_hdf(image_id):\n",
    "    hdf = glob(hdf_dir + '*' + image_id + '*')\n",
    "    for h in hdf:\n",
    "        os.remove(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "624a7b2e-fcc0-4bab-aed4-876542110373",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "image_index['converted'] = False\n",
    "\n",
    "for tile in tiles_to_process:\n",
    "    print(tile)\n",
    "    if tile == \"15SVR\": ## remove edge case\n",
    "        continue\n",
    "        \n",
    "    ## set initial spatial threshold\n",
    "    temp_thres = 100\n",
    "\n",
    "    cand_images = image_index[(image_index.tile == tile) &(image_index.spatial_coverage == 100) & (image_index.cloud_coverage <= cloud_thres)]\n",
    "    print(cand_images)\n",
    "    print(len(cand_images))\n",
    "    if len(cand_images) < 3:\n",
    "        temp_thres = 90\n",
    "        cand_images = image_index[(image_index.tile == tile) &(image_index.spatial_coverage >= temp_thres) & (image_index.cloud_coverage <= cloud_thres)]\n",
    "        print(len(cand_images))\n",
    "    if len(cand_images) < 3:\n",
    "        temp_thres = 80\n",
    "        cand_images = image_index[(image_index.tile == tile) &(image_index.spatial_coverage >= temp_thres) & (image_index.cloud_coverage <= cloud_thres)]\n",
    "        print(len(cand_images))\n",
    "    if len(cand_images) < 3:\n",
    "        temp_thres = 70\n",
    "        cand_images = image_index[(image_index.tile == tile) &(image_index.spatial_coverage >= temp_thres) & (image_index.cloud_coverage <= cloud_thres)]\n",
    "        print(len(cand_images))\n",
    "    if len(cand_images) < 3:\n",
    "        temp_thres = 60\n",
    "        cand_images = image_index[(image_index.tile == tile) &(image_index.spatial_coverage >= temp_thres) & (image_index.cloud_coverage <= cloud_thres)]\n",
    "        print(len(cand_images))\n",
    "    if len(cand_images) < 3:\n",
    "        temp_thres = 50\n",
    "        cand_images = image_index[(image_index.tile == tile) &(image_index.spatial_coverage >= temp_thres) & (image_index.cloud_coverage <= cloud_thres)]\n",
    "        print(len(cand_images))\n",
    "    if len(cand_images) < 3:\n",
    "        print(tile + ' skipped')\n",
    "        continue\n",
    "    print('final spatial threshold ' + str(temp_thres))\n",
    "\n",
    "    # if len(cand_images) < 4:\n",
    "    #     print(tile + ' skipped')\n",
    "    #     continue\n",
    "    first_image, first_image_id, cand_images = convert_first_date(cand_images)\n",
    "    last_image, last_image_id, last_images = convert_last_date(cand_images)\n",
    "    middle_image, middle_image_id, middle_images = convert_middle_date(cand_images)\n",
    "\n",
    "    selected_images = pd.concat([first_image, middle_image, last_image], ignore_index = True)\n",
    "    #print(selected_images)\n",
    "    \n",
    "    image_index.loc[image_index.image_id == first_image_id  , 'converted'] = True\n",
    "    image_index.loc[image_index.image_id == middle_image_id  , 'converted'] = True\n",
    "    image_index.loc[image_index.image_id == last_image_id  , 'converted'] = True\n",
    "\n",
    "\n",
    "    assert len(selected_images) == 3\n",
    "    assert len(selected_images.image_id.unique()) == 3\n",
    "    \n",
    "    ## update tile tracker\n",
    "    tile_tracker = pd.read_csv(tile_tracker_csv)\n",
    "    tile_tracker.loc[tile_tracker.tile == tile , 'tif_convert'] = True\n",
    "    tile_tracker.loc[tile_tracker.tile == tile , 'spatial_cov'] = temp_thres\n",
    "\n",
    "    tile_tracker.to_csv(tile_tracker_csv, index=False)\n",
    "    \n",
    "    # images_to_delete = image_index[(image_index.tile == tile) & (~image_index.image_id.isin([first_image_id, middle_image_id, last_image_id]))]\n",
    "    # a = images_to_delete.image_id.tolist()    \n",
    "    # for x in a:\n",
    "    #     delete_hdf(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3e51d72-a722-41ba-8fb9-facb4ae50f7a",
   "metadata": {},
   "source": [
    "3. reproject selected cog to cdl crs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb1f9b1-649d-4e25-84d4-8d76b1e2f4f9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def reproject_hls_to_cdl(scene_folder,\n",
    "                         bands = [\"B02\", \"B03\", \"B04\", \"B8A\", \"B11\", \"B12\", \"QA\"],\n",
    "                         cdl_file = \"/data/2021_30m_cdls_clipped.tif\"):\n",
    "    \n",
    "    \"\"\"\n",
    "    This function receives the path to a folder that contains all GeoTIFF files (for various bands)\n",
    "    of a HLS scene, and reprojects those to the target CDL CRS and grid. \n",
    "    \n",
    "    Assumptions:\n",
    "    - scene_folder has a file structure like: \".../<scene_id>/<scene_id>.<band_id>.tiff\n",
    "    - scene_folder should not have a \"/\" at the end\n",
    "    \n",
    "    Inputs:\n",
    "    - scene_folder: is the path to the folder that contains HLS GeoTIFF files for all bands of HLS\n",
    "    - bands: list of bands of HLS that should be reprojected (default is all bands)\n",
    "    - cdl_file: contains the path to the clipped CDL GeoTIFF file\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    for band in bands:\n",
    "        xds = xarray.open_rasterio(f\"{scene_folder}/{scene_folder.split('/')[-1]}.{band}.tif\")\n",
    "        cdl = xarray.open_rasterio(cdl_file)\n",
    "        xds_new = xds.rio.reproject_match(cdl, resampling = Resampling.bilinear)\n",
    "        xds_new.rio.to_raster(raster_path = f\"{scene_folder}/{scene_folder.split('/')[-1]}.{band}.5070.tif\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94bdb835-d28d-49c1-a9ea-7b3d27d0bbfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "tile_tracker = pd.read_csv(tile_tracker_csv)\n",
    "#tiles_already_converted = set([i[19:24] for i in tiles_already_converted])\n",
    "tiles_to_reproject = tile_tracker[(tile_tracker.exclude == False) & (tile_tracker.tif_convert == True) & (tile_tracker.tif_reproject == False) ].tile.unique()\n",
    "#tiles_to_reproject = tiles_to_reproject[0:3]\n",
    "tiles_to_reproject\n",
    "\n",
    "# (image_index[image_index.converted == True]).reset_index(drop = True)\n",
    "# print(selected_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce62131-921c-468a-b655-63d516f9f073",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bad_columns = tile_tracker.columns.tolist()\n",
    "# bad_columns = [column for column in bad_columns if 'Unnamed' in column]\n",
    "# print(bad_columns)\n",
    "# tile_tracker = tile_tracker.drop(bad_columns, axis=1)\n",
    "# tile_tracker.to_csv(tile_tracker_csv, index=False)\n",
    "# tile_tracker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97b9a698-3ebf-43e8-8ddc-4b36662413c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chip_df = pd.read_csv(chip_csv)\n",
    "# bad_columns = chip_df.columns.tolist()\n",
    "# bad_columns = [column for column in bad_columns if 'Unnamed' in column]\n",
    "# print(bad_columns)\n",
    "# chip_df = chip_df.drop(bad_columns, axis=1)\n",
    "# chip_df.to_csv(chip_df_csv, index=False)\n",
    "# chip_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f85d47ed-80dc-4a91-861e-8c7795957ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tile_tracker = pd.read_csv(tile_tracker_csv)\n",
    "tile_tracker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "820e9b7d-2fde-48cd-8dd5-fb5976c7ec8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for tile in tiles_to_reproject:\n",
    "    selected_images = glob(tif_dir + '*')\n",
    "    #print(selected_images)\n",
    "    selected_images = [image for image in selected_images if image[19:24] == tile]\n",
    "    print(selected_images)\n",
    "        ## reproject to cdl\n",
    "    for k in range(len(selected_images)):\n",
    "        image_id = selected_images[k]\n",
    "        print(image_id)\n",
    "        reproject_hls_to_cdl(image_id)\n",
    "    tile_tracker = pd.read_csv(tile_tracker_csv)\n",
    "    tile_tracker.loc[tile_tracker.tile == tile , 'tif_reproject'] = True\n",
    "    tile_tracker.to_csv(tile_tracker_csv, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b49307af-b57a-465f-95b7-0a5ef4e7ee91",
   "metadata": {},
   "source": [
    "4. chipping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1222194e-90cc-45bd-94ec-fe65537ca1c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['14SMC', '14SMB', '14SMA', '14SNC', '14SNB', '14SNA', '14SNE',\n",
       "       '14SND', '14SNF', '14SPA', '14SPC', '14SPE', '14SPD', '14SPF',\n",
       "       '14SQA'], dtype=object)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tile_tracker = pd.read_csv(tile_tracker_csv)\n",
    "tiles_to_chip = tile_tracker[(tile_tracker.exclude == False) & (tile_tracker.tif_reproject == True) & (tile_tracker.chip == False) ].tile.unique()\n",
    "tiles_to_chip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2b179370-848c-4350-ac9e-7d49e201be3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_qa(qa_path, shape,  valid_qa = [0, 4, 32, 36, 64, 68, 96, 100, 128, 132, 160, 164, 192, 196, 224, 228]):\n",
    "    \n",
    "    \"\"\"\n",
    "    This function receives a path to a qa file, and a geometry. It clips the QA file to the geometry. \n",
    "    It returns the number of valid QA pixels in the geometry, and the clipped values.\n",
    "    \n",
    "    Assumptions: The valid_qa values are taken from Ben Mack's post:\n",
    "    https://benmack.github.io/nasa_hls/build/html/tutorials/Working_with_HLS_datasets_and_nasa_hls.html\n",
    "    \n",
    "    Inputs:\n",
    "    - qa_path: full path to reprojected QA tif file\n",
    "    - shape: 'geometry' property of single polygon feature read by fiona\n",
    "    - valid_qa: list of integer values that are 'valid' for QA band.\n",
    "    \n",
    "\n",
    "    \n",
    "    \"\"\"\n",
    "    with rasterio.open(qa_path) as src:\n",
    "        out_image, out_transform = rasterio.mask.mask(src, shape, crop=True)\n",
    "      #  print(out_image.shape)\n",
    "        vals = out_image.flatten()\n",
    "        unique, counts = np.unique(vals, return_counts=True)\n",
    "        qa_df = pd.DataFrame({\"qa_val\" : unique, \"counts\" : counts})\n",
    "        qa_df\n",
    "        qa_df[~ qa_df.qa_val.isin(valid_qa)].sort_values(['counts'], ascending = False)\n",
    "        qa_df['pct'] = (100 *qa_df['counts'])/(224.0 * 224.0)\n",
    "        \n",
    "        bad_qa = qa_df[~ qa_df.qa_val.isin(valid_qa)].sort_values(['counts'], ascending = False)\n",
    "        if len(bad_qa) > 0:\n",
    "            highest_invalid_percent = bad_qa.pct.tolist()[0]\n",
    "        else: \n",
    "            highest_invalid_percent = 0\n",
    "        #ncell = len(vals)\n",
    "        valid_count = sum(x in valid_qa for x in vals)\n",
    "        return(valid_count, highest_invalid_percent, out_image[0])\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f78b1dd6-6465-4a93-bb27-dbcc0555a1f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## set up CDL reclass\n",
    "cdl_class_df = pd.read_csv(cdl_reclass_csv)\n",
    "crop_dict = dict(zip(cdl_class_df.CDL_val, cdl_class_df.new_class_value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5849ef63-172a-4313-a4e7-6a1b5bb4bd40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_reclass(x):\n",
    "    ## binary reclass\n",
    "    crop_classes = [1,2,3,4,5,6,10,11,12,13,14,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,66,67,68,69,70,71,72,74,75,76,77,92,204,205,206,207,208,209,210,211,212,213,214,215,216,217,218,219,220,221,222,223,224,225,226,227,228,229,230,231,232,233,236,237,238,240,241,242,243,244,245,246,247,248,249,250,254]\n",
    "    return(crop_classes.count(x))\n",
    "\n",
    "c_rcl = np.vectorize(crop_reclass)\n",
    "\n",
    "\n",
    "def crop_multi(x):\n",
    "    return(crop_dict[x])\n",
    "\n",
    "\n",
    "c_multi = np.vectorize(crop_multi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "997eec43-0008-4380-9a16-57ac20b68226",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reorder_bands(file_list, band_order = [\"B02\", \"B03\", \"B04\", \"B8A\", \"B11\", \"B12\", \"QA\"]):\n",
    "    reordered = []\n",
    "    for band in band_order:\n",
    "        band_dots = '.' + band + '.'\n",
    "        file_name = [s for s in file_list if band_dots in s]\n",
    "        print('file_name')\n",
    "        print(file_name)\n",
    "        assert (len(file_name) == 1)\n",
    "        reordered.append(file_name[0])\n",
    "    print(reordered)\n",
    "    return(reordered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b5e60745-8c2e-4bcd-8130-2cf76a0e7e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_chip(chip_id, \n",
    "                 chip_tile,\n",
    "                 shape,\n",
    "                 bands = [\"B02\", \"B03\", \"B04\", \"B8A\", \"B11\", \"B12\", \"QA\"]):\n",
    "    \n",
    "    \"\"\"\n",
    "    This function receives a chip id, HLS tile, chip geometry, and a list of bands to process. \n",
    "    \n",
    "    Assumptions:\n",
    "    \n",
    "    Inputs:\n",
    "    - chip_id: string of chip id, e.g. '000_001'\n",
    "    - chip_tile: string of HLS tile , e.g. '15ABC'\n",
    "    - shape: 'geometry' property of single polygon feature read by fiona\n",
    "    \n",
    "    The function writes out a multi-date TIF containing the bands for each of the three image dates for an HLS tile. \n",
    "    The function writes out a multi-date TIF containing the QA bands of each date.\n",
    "    The function writes out a chipped version of CDL. \n",
    "    The function calls check_qa(), which makes assumptions about what QA pixels are valid.\n",
    "    The function returns the number of valid QA pixels at each date, as a tuple.\n",
    "    \n",
    "    \"\"\"\n",
    "    ## get reprojected image paths\n",
    "    selected_image_folders = sorted(glob(f'/data/tif/*T{chip_tile}*'))\n",
    "   # print(selected_image_folders)\n",
    "    \n",
    "    assert len(selected_image_folders) == 3\n",
    "    \n",
    "    first_image_date = selected_image_folders[0][25:32]\n",
    "    second_image_date = selected_image_folders[1][25:32]\n",
    "    third_image_date = selected_image_folders[2][25:32]\n",
    "    \n",
    "    first_date_images = sorted(glob(selected_image_folders[0] + '/*.5070.tif'))\n",
    "    \n",
    "    first_date_images = reorder_bands(first_date_images, band_order = bands)\n",
    "    first_date_qa = [x for x in first_date_images if '.QA.' in x][0]\n",
    "    first_date_images.remove(first_date_qa)\n",
    "    \n",
    "    second_date_images = sorted(glob(selected_image_folders[1] + '/*.5070.tif'))\n",
    "    second_date_images = reorder_bands(second_date_images, band_order = bands)\n",
    "\n",
    "    second_date_qa = [x for x in second_date_images if '.QA.' in x][0]\n",
    "    second_date_images.remove(second_date_qa)\n",
    "    \n",
    "    third_date_images = sorted(glob(selected_image_folders[2] + '/*.5070.tif'))\n",
    "    third_date_images = reorder_bands(third_date_images, band_order = bands)\n",
    "\n",
    "    third_date_qa = [x for x in third_date_images if '.QA.' in x][0]\n",
    "    third_date_images.remove(third_date_qa)\n",
    "    all_date_images = first_date_images + second_date_images + third_date_images\n",
    "    \n",
    "    print('all date images')\n",
    "    print(all_date_images)\n",
    "  #  print(len(all_date_images))\n",
    "\n",
    "\n",
    "    valid_first, bad_pct_first, qa_first = check_qa(first_date_qa, shape)\n",
    "    valid_second, bad_pct_second, qa_second = check_qa(second_date_qa, shape)\n",
    "    valid_third, bad_pct_third, qa_third = check_qa(third_date_qa, shape)\n",
    "    \n",
    "    qa_bands = []\n",
    "    qa_bands.append(qa_first)\n",
    "    qa_bands.append(qa_second)\n",
    "    qa_bands.append(qa_third)\n",
    "    qa_bands = np.array(qa_bands).astype(np.int16)\n",
    "    \n",
    "  #  print(qa_bands.shape)\n",
    "   # print(first_date_qa)\n",
    "    assert len(all_date_images) == 3 * (len(bands) - 1)\n",
    "    \n",
    "    out_bands = []\n",
    "    print('out_bands_loop')\n",
    "    for img in all_date_images:\n",
    "        with rasterio.open(img) as src:\n",
    "            print(img)\n",
    "            out_image, out_transform = rasterio.mask.mask(src, shape, crop=True)\n",
    "            out_meta = src.meta\n",
    "            out_bands.append(out_image[0])\n",
    "    \n",
    "    out_bands = np.array(out_bands)\n",
    "    # print(out_bands.shape)\n",
    "    # print(out_image.shape)\n",
    "\n",
    "    out_meta.update({\"driver\": \"GTiff\",\n",
    "                     \"height\": out_bands.shape[1],\n",
    "                     \"width\": out_bands.shape[2],\n",
    "                     \"count\": out_bands.shape[0],\n",
    "                     \"transform\": out_transform})\n",
    "    \n",
    "    # get NA count for HLS\n",
    "    na_count = sum(out_bands.flatten() == -1000)\n",
    "    \n",
    "    # reclass negative HLS values to 0\n",
    "    out_bands = np.clip(out_bands, 0, None)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # write HLS chip to 'chips'\n",
    "    with rasterio.open(chip_dir + \"chip_\" + str(chip_id) + \"_merged.tif\", \"w\", **out_meta) as dest:\n",
    "        dest.write(out_bands)\n",
    "    # write HLS chip to 'chips_binary'\n",
    "    with rasterio.open(chip_dir_binary + \"chip_\" + str(chip_id) + \"_merged.tif\", \"w\", **out_meta) as dest:\n",
    "        dest.write(out_bands)\n",
    "    # write HLS chip to 'chips_multi'\n",
    "    with rasterio.open(chip_dir_multi + \"chip_\" + str(chip_id) + \"_merged.tif\", \"w\", **out_meta) as dest:\n",
    "        dest.write(out_bands)\n",
    "      \n",
    "    ## write QA bands\n",
    "    out_meta.update({\"driver\": \"GTiff\",\n",
    "                     \"height\": qa_bands.shape[1],\n",
    "                     \"width\": qa_bands.shape[2],\n",
    "                     \"count\": qa_bands.shape[0],\n",
    "                     \"transform\": out_transform})\n",
    "    \n",
    "    with rasterio.open(chip_qa_dir + \"chip_\" + str(chip_id) + \"_qa.tif\", \"w\", **out_meta) as dest:\n",
    "        dest.write(qa_bands)  \n",
    "    \n",
    "        \n",
    "    ## clip cdl to chip\n",
    "    with rasterio.open(\"/data/2021_30m_cdls_clipped.tif\") as src:\n",
    "        out_image, out_transform = rasterio.mask.mask(src, shape, crop=True)\n",
    "        out_meta = src.meta\n",
    "        colormap = src.colormap(1)\n",
    "\n",
    "    out_meta.update({\"driver\": \"GTiff\",\n",
    "                     \"height\": out_image.shape[1],\n",
    "                     \"width\": out_image.shape[2],\n",
    "                     \"transform\": out_transform})\n",
    "    # write CDL chip to 'chips'\n",
    "    with rasterio.open(chip_dir + \"chip_\" + str(chip_id) + \".mask.tif\", \"w\", **out_meta) as dest:\n",
    "        dest.write(out_image)\n",
    "        dest.write_colormap(1, colormap)\n",
    "        \n",
    "        \n",
    "    # write binary  reclassed CDL chip to chips_binary\n",
    "    out_image_binary = c_rcl(out_image).astype(np.uint8)\n",
    "    with rasterio.open(chip_dir_binary + \"chip_\" + str(chip_id) + \".mask.tif\", \"w\", **out_meta) as dest:\n",
    "        dest.write(out_image_binary)\n",
    "        dest.write_colormap(1, colormap)\n",
    "        \n",
    "    # write multiclass  reclassed CDL chip to chips_multi\n",
    "    out_image_multi = c_multi(out_image).astype(np.uint8)\n",
    "    with rasterio.open(chip_dir_multi + \"chip_\" + str(chip_id) + \".mask.tif\", \"w\", **out_meta) as dest:\n",
    "        dest.write(out_image_multi)\n",
    "        dest.write_colormap(1, colormap)\n",
    "    \n",
    "    \n",
    "    return(valid_first,\n",
    "           valid_second,\n",
    "           valid_third, \n",
    "           bad_pct_first,\n",
    "           bad_pct_second,\n",
    "           bad_pct_third,\n",
    "           qa_first,\n",
    "           qa_second,\n",
    "           qa_third,\n",
    "           na_count,\n",
    "           first_image_date,\n",
    "           second_image_date,\n",
    "           third_image_date)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e6df41-29d7-4a0b-917b-0628c25da424",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## process chips\n",
    "chip_df = pd.read_csv(chip_csv)\n",
    "\n",
    "for tile in tiles_to_chip:\n",
    "    print(tile)\n",
    "    chips_to_process = chip_df[chip_df.tile == tile].reset_index(drop = True)\n",
    "    for k in range(len(chips_to_process)):\n",
    "        current_id = chips_to_process.chip_id[k]\n",
    "        chip_tile = chips_to_process.tile[k]\n",
    "    #    print(current_id)\n",
    "        chip_index = chip_ids.index(current_id)\n",
    "\n",
    "        chip_feature = chips['features'][chip_index]\n",
    "\n",
    "        shape = [chip_feature['geometry']]\n",
    "\n",
    "        ## do we want to scale/clip reflectances?\n",
    "\n",
    "        valid_first,  valid_second, valid_third, bad_pct_first, bad_pct_second, bad_pct_third, qa_first, qa_second, qa_third, na_count, first_image_date, second_image_date, third_image_date = process_chip(current_id, chip_tile, shape)\n",
    "\n",
    "        chip_df_index = chip_df.index[chip_df['chip_id'] == current_id].tolist()[0]\n",
    "        chip_df.at[chip_df_index, 'valid_first'] = valid_first\n",
    "        chip_df.at[chip_df_index, 'valid_second'] = valid_second\n",
    "        chip_df.at[chip_df_index, 'valid_third'] = valid_third\n",
    "        chip_df.at[chip_df_index, 'bad_pct_first'] = bad_pct_first\n",
    "        chip_df.at[chip_df_index, 'bad_pct_second'] = bad_pct_second\n",
    "        chip_df.at[chip_df_index, 'bad_pct_third'] = bad_pct_third\n",
    "        chip_df.at[chip_df_index, 'first_image_date'] = first_image_date\n",
    "        chip_df.at[chip_df_index, 'second_image_date'] = second_image_date\n",
    "        chip_df.at[chip_df_index, 'third_image_date'] = third_image_date\n",
    "        chip_df['bad_pct_max'] = chip_df[['bad_pct_first', 'bad_pct_second', 'bad_pct_third']].max(axis=1)\n",
    "        chip_df.at[chip_df_index, 'na_count'] = na_count\n",
    "    tile_tracker = pd.read_csv(tile_tracker_csv)\n",
    "    tile_tracker.loc[tile_tracker.tile == tile , 'chip'] = True\n",
    "    tile_tracker.to_csv(tile_tracker_csv, index=False)\n",
    "chip_df.to_csv(chip_csv, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f356827-1720-4d4b-9187-11e4c23e3108",
   "metadata": {},
   "source": [
    "5. filter chips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5168c2d9-ec69-4a3d-95ab-a0bb2e248a4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['14SMF', '14SME', '14SMD', '14SMC', '14SMB', '14SMA', '14SNC',\n",
       "       '14SNB', '14SNA', '14SNE', '14SND', '14SNF', '14SPA', '14SPC',\n",
       "       '14SPE', '14SPD', '14SPF', '14SQA', '14SQC', '14SQF', '14SQE',\n",
       "       '14SQD', '15STA', '15STV', '15STU', '15STT', '15STS', '15STR',\n",
       "       '15SUA', '15SUV', '15SUU', '15SUS', '15SUR', '15SVA', '15SVV',\n",
       "       '15SVU', '15SVT', '15SVS'], dtype=object)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tile_tracker = pd.read_csv(tile_tracker_csv)\n",
    "tiles_to_filter = tile_tracker[(tile_tracker.exclude == False) & (tile_tracker.chip == True) & (tile_tracker.filter_chips == False) ].tile.unique()\n",
    "tiles_to_filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4ae005b8-8ba1-42ac-838d-cf6eb3d38ad1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14SMF\n",
      "35\n",
      "14SME\n",
      "36\n",
      "14SMD\n",
      "11\n",
      "14SMC\n",
      "16\n",
      "14SMB\n",
      "1\n",
      "14SMA\n",
      "8\n",
      "14SNC\n",
      "208\n",
      "14SNB\n",
      "160\n",
      "14SNA\n",
      "88\n",
      "14SNE\n",
      "181\n",
      "14SND\n",
      "175\n",
      "14SNF\n",
      "135\n",
      "14SPA\n",
      "88\n",
      "14SPC\n",
      "175\n",
      "14SPE\n",
      "186\n",
      "14SPD\n",
      "136\n",
      "14SPF\n",
      "144\n",
      "14SQA\n",
      "81\n",
      "14SQC\n",
      "114\n",
      "14SQF\n",
      "126\n",
      "14SQE\n",
      "144\n",
      "14SQD\n",
      "106\n",
      "15STA\n",
      "64\n",
      "15STV\n",
      "138\n",
      "15STU\n",
      "94\n",
      "15STT\n",
      "126\n",
      "15STS\n",
      "146\n",
      "15STR\n",
      "69\n",
      "15SUA\n",
      "160\n",
      "15SUV\n",
      "184\n",
      "15SUU\n",
      "158\n",
      "15SUS\n",
      "102\n",
      "15SUR\n",
      "66\n",
      "15SVA\n",
      "122\n",
      "15SVV\n",
      "93\n",
      "15SVU\n",
      "152\n",
      "15SVT\n",
      "105\n",
      "15SVS\n",
      "106\n"
     ]
    }
   ],
   "source": [
    "chip_df = pd.read_csv(chip_csv)\n",
    "\n",
    "for tile in tiles_to_filter:\n",
    "    print(tile)\n",
    "    filtered_chips = chip_df[(chip_df.tile == tile) & (chip_df.bad_pct_max < 5) & (chip_df.na_count == 0)].chip_id.tolist()\n",
    "    print(len(filtered_chips))\n",
    "    for chip_id in filtered_chips:\n",
    "        chip_files = glob('/data/chips/*' + chip_id + '*')\n",
    "        for file in chip_files:\n",
    "            name = file.split('/')[-1]\n",
    "            shutil.copyfile(file, '/data/chips_filtered/' + name)\n",
    "        chip_files_b = glob('/data/chips_binary/*' + chip_id + '*')\n",
    "        for file in chip_files_b:\n",
    "            name = file.split('/')[-1]\n",
    "            shutil.copyfile(file, '/data/chips_binary_filtered/' + name)\n",
    "        chip_files_multi = glob('/data/chips_multi/*' + chip_id + '*')\n",
    "        for file in chip_files_multi:\n",
    "            name = file.split('/')[-1]\n",
    "            shutil.copyfile(file, '/data/chips_multi_filtered/' + name)\n",
    "    \n",
    "    tile_tracker = pd.read_csv(tile_tracker_csv)\n",
    "    tile_tracker.loc[tile_tracker.tile == tile , 'filter_chips'] = True\n",
    "    tile_tracker.to_csv(tile_tracker_csv, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71c14533-4be1-4636-a237-c749fff3497a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# valid_qa = [0, 4, 32, 36, 64, 68, 96, 100, 128, 132, 160, 164, 192, 196, 224, 228]\n",
    "# qa_df_all['valid'] = qa_df_all.qa_val.isin(valid_qa)\n",
    "# qa_df_all\n",
    "# qa_df_all.to_csv(root_path + \"_\" + str(ct) + 'qa_vals.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "232e31f6-cb5b-4e38-85a1-aeb5f59d8efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# valid_qa = [0, 4, 32, 36, 64, 68, 96, 100, 128, 132, 160, 164, 192, 196, 224, 228]\n",
    "\n",
    "# qa_df_all = pd.DataFrame(columns = [\"qa_val\", \"counts\", \"chip_id\", 'date'])\n",
    "\n",
    "# for chip in qa_chips[0:2]:\n",
    "#     vals = xarray.open_rasterio(chip)\n",
    "#     for k in range(3):\n",
    "#         date_vals = vals.data[k, :, :]\n",
    "#         unique, counts = np.unique(date_vals, return_counts=True)\n",
    "#         qa_df = pd.DataFrame({\"qa_val\" : unique, \"counts\" : counts})\n",
    "#         qa_df['pct'] = (100 *qa_df['counts'])/(224.0 * 224.0)\n",
    "#         qa_df['chip_id'] = chip.split('/')[-1][12:19]\n",
    "#         qa_df['date'] = str(k)\n",
    "#         qa_df_all = pd.concat([qa_df_all, qa_df])\n",
    "        \n",
    "# #qa_df_all.to_csv(root_path + 'qa_vals_date.csv')\n",
    "# qa_df_all[~ qa_df_all.qa_val.isin(valid_qa)].sort_values(['counts'], ascending = False).pct.tolist()[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "220ddeb4-abb5-4c2f-a7c4-af082898950f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
