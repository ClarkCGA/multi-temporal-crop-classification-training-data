{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "106a866f-39a2-4b20-8ba6-f5ec3d556561",
   "metadata": {},
   "outputs": [],
   "source": [
    "yoi = [2020]\n",
    "toi = ['15RTN', '15RTP', '14RNS']\n",
    "root_path = \"C:/Users/mcecil/CGA/CDL/\"\n",
    "spath = root_path + f\"CDL_HLS_dataframe{yoi[0]}.csv\"\n",
    "url_file = root_path + f'CDL_HLS_dataframe{yoi[0]}.csv'\n",
    "index_file = root_path + f'mask_index_{yoi[0]}.csv'\n",
    "kml_file = root_path + 'sentinel_tile_grid.kml'\n",
    "geojson_file = root_path + 'aoi.geojson'  ## chip file, lat-long\n",
    "geojson_rpj_file = root_path + 'aoi_rpj.geojson'\n",
    "hdf_dir = root_path + 'hdf/'\n",
    "tiff_dir = root_path + 'tif/'\n",
    "mask_dir = root_path + 'masks/'\n",
    "\n",
    "tile_src_path = root_path + \"tile_src.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "890a508e-f65d-4df7-84e9-172ba05b627d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Switch plotting backend\n",
    "import matplotlib\n",
    "matplotlib.use('agg')\n",
    "\n",
    "# Import required modules\n",
    "#import cartopy.crs as ccrs\n",
    "import os\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import geopandas\n",
    "#import pyhdf\n",
    "import rasterio\n",
    "import rasterio.mask\n",
    "import matplotlib.pyplot as pp\n",
    "import nasa_hls\n",
    "import numpy as np\n",
    "import time\n",
    "import fiona\n",
    "from pathlib import Path\n",
    "import time\n",
    "import urllib.request as urlreq\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94492d6e-242f-49aa-a8a8-ca1ccbbb3ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the HLS tiles and place there coordinates into a numpy array for processing later\n",
    "fiona.drvsupport.supported_drivers['KML'] = 'rw'\n",
    "tile_src = geopandas.read_file(kml_file, driver='KML')\n",
    "tile_name = []\n",
    "tile_x = []\n",
    "tile_y = []\n",
    "for tile_ind in range(tile_src.shape[0]):\n",
    "    tile_name.append(tile_src.iloc[tile_ind].Name)\n",
    "    tile_x.append(tile_src.iloc[tile_ind].geometry.centroid.x)\n",
    "    tile_y.append(tile_src.iloc[tile_ind].geometry.centroid.y)\n",
    "tile_name = np.array(tile_name)\n",
    "tile_x = np.array(tile_x)\n",
    "tile_y = np.array(tile_y)\n",
    "tile_src = pd.concat([tile_src, tile_src.bounds], axis = 1)\n",
    "#del tile_src\n",
    "tile_src.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae1ee1ab-d276-4bd8-ad3d-fe29d7182664",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the HLS query csv for this year\n",
    "qfn = open(url_file)\n",
    "qtile = []\n",
    "qyear = []\n",
    "qmonth = []\n",
    "qday = []\n",
    "qdate = []\n",
    "qurl = []\n",
    "for i, line in enumerate(qfn):\n",
    "    if (i == 0): # Skip header\n",
    "        continue\n",
    "    dummy = line.split(\",\")\n",
    "    qtile.append(dummy[2])\n",
    "    qdate.append(dummy[3])\n",
    "    qurl.append(dummy[4])\n",
    "    \n",
    "    qday.append(int(dummy[3].split(\"-\")[2]))\n",
    "    qmonth.append(int(dummy[3].split(\"-\")[1]))\n",
    "    qyear.append(int(dummy[3].split(\"-\")[0]))\n",
    "qfn.close()\n",
    "qdate = np.array(qdate)\n",
    "qtile = np.array(qtile)\n",
    "qurl = np.array(qurl)\n",
    "qmonth = np.array(qmonth)\n",
    "qyear = np.array(qyear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db83c256-6c7b-4f86-86e0-afcbe0c1e1f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the index file\n",
    "# fn_index = open(index_file, \"w\")\n",
    "# fn_index.write('EventID,Name,FireDate,MaskDate,BurnAcres,Tile,MaskFile')\n",
    "# fn_index.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ecd3a8f-73b7-400c-a0ae-0cff713066d3",
   "metadata": {},
   "source": [
    "Load geojson used for query. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f600d0ca-467b-4ec2-a8f3-8f4b76434d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "aoi_src = geopandas.read_file(geojson_file) ## this could be a list\n",
    "nfeatures = aoi_src.shape[0]\n",
    "aoi_src = pd.concat([aoi_src, aoi_src.bounds], axis = 1)\n",
    "aoi_src['centroid_x'] = aoi_src.centroid.x\n",
    "aoi_src['centroid_y'] = aoi_src.centroid.y\n",
    "aoi_src.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0319bc23-2006-4d35-b46e-d6d037a70f76",
   "metadata": {},
   "source": [
    "Loop through aoi_src (chips), associate closest tile, add ID field, download hdf (WORKING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d81f4f7d-d6b9-46b9-a4a5-f58f16b9700f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for aoi_ind in range(nfeatures):\n",
    "    aoi_row = aoi_src.iloc[aoi_ind]\n",
    "    print(aoi_row)\n",
    "    print(aoi_src.bounds)\n",
    "    aoi_x = float(aoi_row.centroid_x)\n",
    "    aoi_y = float(aoi_row.centroid_y)\n",
    "        \n",
    "    # Identify what tile the burn scar is in\n",
    "    s = (tile_x-aoi_x)**2+(tile_y-aoi_y)**2\n",
    "    tname = tile_name[np.argmin(s)]\n",
    "    print(s)\n",
    "    print(tname)\n",
    "    aoi_src.at[aoi_ind, 'tile'] = tname\n",
    "    aoi_src.at[aoi_ind, 'chip_id'] = 'chip_' + str(aoi_ind)\n",
    "    \n",
    "    \n",
    "    # Subset potential images based on tile and date\n",
    "    tile_mask = qtile == tname\n",
    "    # Subset based on date\n",
    "    date_mask1 = (qyear == yoi[0])\n",
    "  #  diff = (qmonth-bs_month)%12\n",
    "   # date_mask2 = ((diff <= month_threshold[1]) & (diff >= month_threshold[0]))\n",
    "    date_mask = date_mask1  #& date_mask2\n",
    "    \n",
    "    mask = tile_mask & date_mask\n",
    "    candidate_dates = qdate[mask]\n",
    "    candidate_tiles = qtile[mask]\n",
    "    candidate_urls = qurl[mask]\n",
    "    # Loop through images and check cloud cover\n",
    "    for cd, ct, cu in zip(candidate_dates, candidate_tiles, candidate_urls):     \n",
    "        local_name = cu.split('/')[-1].replace(\"\\n\", \"\")\n",
    "        # Download a candidate file to check cloud cover\n",
    "        try:\n",
    "            urlreq.urlretrieve(cu, filename=hdf_dir+local_name)\n",
    "        except:\n",
    "            continue\n",
    "#aoi_src.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "104f3200-7d45-4911-91ea-527a67e56f27",
   "metadata": {},
   "source": [
    "Extract cloud cover and spatial coverage from metadata (NOT WORKING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4fa0074-2fca-42f6-8ed3-545a8e05e3f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for aoi_ind in range(nfeatures):\n",
    "    aoi_row = aoi_src.iloc[aoi_ind]\n",
    "    print(aoi_row)\n",
    "    print(aoi_src.bounds)\n",
    "    aoi_x = float(aoi_row.centroid_x)\n",
    "    aoi_y = float(aoi_row.centroid_y)\n",
    "        \n",
    "    # Identify what tile the burn scar is in\n",
    "    s = (tile_x-aoi_x)**2+(tile_y-aoi_y)**2\n",
    "    tname = tile_name[np.argmin(s)]\n",
    "    print(s)\n",
    "    print(tname)\n",
    "    aoi_src.at[aoi_ind, 'tile'] = tname\n",
    "    aoi_src.at[aoi_ind, 'chip_id'] = 'chip_' + str(aoi_ind)\n",
    "    \n",
    "    # Subset potential images based on tile and date\n",
    "    tile_mask = qtile == tname\n",
    "    # Subset based on date\n",
    "    date_mask1 = (qyear == yoi[0])\n",
    "  #  diff = (qmonth-bs_month)%12\n",
    "   # date_mask2 = ((diff <= month_threshold[1]) & (diff >= month_threshold[0]))\n",
    "    date_mask = date_mask1  #& date_mask2\n",
    "    \n",
    "    mask = tile_mask & date_mask\n",
    "    candidate_dates = qdate[mask]\n",
    "    candidate_tiles = qtile[mask]\n",
    "    candidate_urls = qurl[mask]\n",
    "    # Loop through images and check cloud cover\n",
    "    for cd, ct, cu in zip(candidate_dates, candidate_tiles, candidate_urls):\n",
    "       # print(cu)\n",
    "        local_name = cu.split('/')[-1].replace(\"\\n\", \"\")\n",
    "        \n",
    "        if local_name != \"HLS.S30.T14RNS.2020117.v1.4.hdf\":\n",
    "            continue\n",
    "        # Download a candidate file to check cloud cover\n",
    "        try:\n",
    "            md = nasa_hls.get_metadata_from_hdf(hdf_dir+local_name, fields=['cloud_cover', 'spatial_coverage'])\n",
    "            print(md)\n",
    "       #     if ((md[\"cloud_cover\"] < cldfrac_threshold) and (md['spatial_coverage'] > spacecov_threshold)):\n",
    "        except:\n",
    "            continue\n",
    "        \n",
    "aoi_src.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd8e2252-4cf9-44ea-bb3c-84628f7d1f10",
   "metadata": {},
   "source": [
    "Convert HDF to single-layer TIF (Using HLS library, DOES NOT WORK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9654ff4a-7955-4dbe-9978-8b6f61ac5d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for aoi_ind in range(nfeatures):\n",
    "    aoi_row = aoi_src.iloc[aoi_ind]\n",
    "    print(aoi_row)\n",
    "    print(aoi_src.bounds)\n",
    "    aoi_x = float(aoi_row.centroid_x)\n",
    "    aoi_y = float(aoi_row.centroid_y)\n",
    "        \n",
    "    # Identify what tile the burn scar is in\n",
    "    s = (tile_x-aoi_x)**2+(tile_y-aoi_y)**2\n",
    "    tname = tile_name[np.argmin(s)]\n",
    "    print(s)\n",
    "    print(tname)\n",
    "    aoi_src.at[aoi_ind, 'tile'] = tname\n",
    "    aoi_src.at[aoi_ind, 'chip_id'] = 'chip_' + str(aoi_ind)\n",
    "    \n",
    "    # Subset potential images based on tile and date\n",
    "    tile_mask = qtile == tname\n",
    "    # Subset based on date\n",
    "    date_mask1 = (qyear == yoi[0])\n",
    "  #  diff = (qmonth-bs_month)%12\n",
    "   # date_mask2 = ((diff <= month_threshold[1]) & (diff >= month_threshold[0]))\n",
    "    date_mask = date_mask1  #& date_mask2\n",
    "    \n",
    "    mask = tile_mask & date_mask\n",
    "    candidate_dates = qdate[mask]\n",
    "    candidate_tiles = qtile[mask]\n",
    "    candidate_urls = qurl[mask]\n",
    "    # Loop through images and check cloud cover\n",
    "    for cd, ct, cu in zip(candidate_dates, candidate_tiles, candidate_urls):\n",
    "        print(cu)\n",
    "        local_name = cu.split('/')[-1].replace(\"\\n\", \"\")\n",
    "        \n",
    "        if local_name != \"HLS.S30.T14RNS.2020117.v1.4.hdf\":\n",
    "            continue\n",
    "        \n",
    "        nasa_hls.convert_hdf2tiffs(Path(hdf_dir+local_name), Path(tiff_dir))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26661253-3d2e-4441-83f4-7f60c19838b9",
   "metadata": {},
   "source": [
    "Convert HDF to single-layer TIF (Mike's way - works)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "177418f1-9c60-493d-9f88-106c992964d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import osgeo\n",
    "from osgeo import gdal\n",
    "import glob\n",
    "from subprocess import Popen, PIPE\n",
    "\n",
    "bands = ['B01', 'B02', 'B03', 'B04', 'B05', 'B06', 'B07', 'B08', 'B8A', 'B09', 'B10', 'B11', 'B12', 'QA']\n",
    "\n",
    "\n",
    "hdf_files = glob.glob(hdf_dir + \"*.hdf\")\n",
    "for hdf_file in hdf_files:\n",
    "    print(hdf_file)\n",
    "    file_name = hdf_file.split('\\\\')[-1].replace('.hdf', '')\n",
    "    if file_name != \"HLS.S30.T14RNS.2020117.v1.4\":\n",
    "        continue\n",
    "    print('valid')\n",
    "    for long_band_name in bands:\n",
    "        cmd = f'gdal_translate HDF4_EOS:EOS_GRID:\"' + hdf_file + '\":Grid:' + long_band_name + ' ' + tiff_dir + file_name + '__' + long_band_name + '.tif'\n",
    "      #  print(cmd)\n",
    "        p = Popen(cmd, stdout=PIPE, shell=True)\n",
    "       # print(p)\n",
    "        output, err = p.communicate()\n",
    "        #print(output)\n",
    "        #print(err)\n",
    "  #  nasa_hls.convert_hdf2tif_dirfs(Path(hdf_file), Path(tif_dir_dir))\n",
    "    # ds = gdal.Open(hdf_file)\n",
    "    # ds_t = gdal.Translate('output.tif', hdf_file)\n",
    "#ds = None\n",
    "#print(ds)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64823ba5-0b08-4038-8ab2-d3419e42c3c1",
   "metadata": {},
   "source": [
    "Create mask tif files (1 if valid data, 0 if not). Uses geojson (lat long) to mask. Does not crop. (WORKS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e9c9262-9312-448e-8d84-d1cd5ca540e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for aoi_ind in range(nfeatures):\n",
    "    aoi_row = aoi_src.iloc[aoi_ind]\n",
    "    print(aoi_row)\n",
    "    print(aoi_src.bounds)\n",
    "    aoi_x = float(aoi_row.centroid_x)\n",
    "    aoi_y = float(aoi_row.centroid_y)\n",
    "        \n",
    "    # Identify what tile the burn scar is in\n",
    "    s = (tile_x-aoi_x)**2+(tile_y-aoi_y)**2\n",
    "    tname = tile_name[np.argmin(s)]\n",
    "    print(s)\n",
    "    print(tname)\n",
    "    aoi_src.at[aoi_ind, 'tile'] = tname\n",
    "    aoi_src.at[aoi_ind, 'chip_id'] = 'chip_' + str(aoi_ind)\n",
    "    \n",
    "    # Subset potential images based on tile and date\n",
    "    tile_mask = qtile == tname\n",
    "    # Subset based on date\n",
    "    date_mask1 = (qyear == yoi[0])\n",
    "  #  diff = (qmonth-aoi_month)%12\n",
    "   # date_mask2 = ((diff <= month_threshold[1]) & (diff >= month_threshold[0]))\n",
    "    date_mask = date_mask1  #& date_mask2\n",
    "    \n",
    "    mask = tile_mask & date_mask\n",
    "    candidate_dates = qdate[mask]\n",
    "    candidate_tiles = qtile[mask]\n",
    "    candidate_urls = qurl[mask]\n",
    "    # Loop through images and check cloud cover\n",
    "    for cd, ct, cu in zip(candidate_dates, candidate_tiles, candidate_urls):\n",
    "       # print(cu)\n",
    "        local_name = cu.split('/')[-1].replace(\"\\n\", \"\")\n",
    "        print(local_name)\n",
    "        # if local_name != \"HLS.S30.T14RNS.2020345.v1.4.hdf\":\n",
    "        #     continue\n",
    "        # print('valid')\n",
    "        # Download a candidate file to check cloud cover\n",
    "        # try:\n",
    "        #     urlreq.urlretrieve(cu, filename=hdf_dir+local_name)\n",
    "        # except:\n",
    "        #     continue\n",
    "    \n",
    "        # Check cloud cover and spacial coverage\n",
    "      #  try:\n",
    "            # md = nasa_hls.get_metadata_from_hdf(hdf_dir+local_name, fields=['cloud_cover', 'spatial_coverage'])\n",
    "            # if ((md[\"cloud_cover\"] < cldfrac_threshold) and (md['spatial_coverage'] > spacecov_threshold)):\n",
    "            \n",
    "                # Convert to a geotiff            \n",
    "                # nasa_hls.convert_hdf2tiffs(Path(hdf_dir+local_name), Path(tiff_dir))\n",
    "\n",
    "            # Open the new geotiff\n",
    "        date = datetime.strptime(cd, '%Y-%m-%d')\n",
    "        doy = (date-datetime(date.year, 1, 1)).days+1\n",
    "        fp = f'HLS.S30.T{ct}.{date.year}{doy:03d}.v1.4'\n",
    "        src = rasterio.open(f'{tiff_dir}/{fp}__B04.tif')\n",
    "        red = src.read(1)\n",
    "        print(src)\n",
    "        print(red)\n",
    "\n",
    "        # Open NIR too to compute NDVI\n",
    "        nir_src = rasterio.open(f'{tiff_dir}/{fp}__B08.tif')\n",
    "        nir = nir_src.read(1)\n",
    "\n",
    "        \n",
    "        qa_src = rasterio.open(f'{tiff_dir}/{fp}__QA.tif')\n",
    "        qa = qa_src.read(1)\n",
    "        \n",
    "        print(qa_src)\n",
    "        print(qa)\n",
    "        \n",
    "        # Compute NDVI\n",
    "        red = np.clip(red/15000, 0, 1)\n",
    "        nir = np.clip(nir/15000, 0, 1)\n",
    "        ndvi = np.clip((nir-red)/(nir+red), 0, 1)\n",
    "\n",
    "        # Convert burn scar shape to proper geometry\n",
    "        aoi_shape = aoi_src.to_crs(src.crs).iloc[aoi_ind]\n",
    "\n",
    "        print(aoi_shape)\n",
    "        # Build the mask\n",
    "        out_image, out_transform = rasterio.mask.mask(src, [aoi_shape.geometry], crop=False)\n",
    "        print(np.min(out_image), np.max(out_image))\n",
    "        out_image = np.clip(out_image, 0, 1)\n",
    "        print(np.min(out_image), np.max(out_image))\n",
    "\n",
    "        out_meta = src.meta\n",
    "        out_meta.update({'driver':'GTiff', 'height':out_image.shape[1],\n",
    "            'width':out_image.shape[2], 'transform':out_transform})\n",
    "\n",
    "        with rasterio.open(f'{mask_dir}/{fp}.mask.tif', 'w', **out_meta) as dest:\n",
    "            dest.write(out_image)\n",
    "\n",
    "        # Plot the original and mask\n",
    "        fig, [ax1, ax2] = pp.subplots(ncols=2)\n",
    "        ax1.imshow(ndvi, cmap='YlGn', vmin=0, vmax=1)\n",
    "        ax2.imshow(np.squeeze(out_image), cmap='Greys')\n",
    "        pp.savefig(f'{mask_dir}/{fp}.mask.jpg')\n",
    "        pp.close()\n",
    "\n",
    "        # Save in index file\n",
    "#         fn_index = open(index_file, 'a')\n",
    "#         fn_index.write(f'\\n{aoi_row.Event_ID},{aoi_row.Incid_Name},{aoi_row.Ig_Date},{cd},{aoi_row.BurnBndAc},{ct},{fp}.mask.tif')\n",
    "#         fn_index.close()\n",
    "\n",
    "#         # Move onto the next burn scar\n",
    "#         del src\n",
    "#         del nir_src\n",
    "       # break\n",
    "        # except:\n",
    "        #     print('error')\n",
    "        #     continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b483e1-aae4-4918-9cb2-0eb5fb253ed2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
