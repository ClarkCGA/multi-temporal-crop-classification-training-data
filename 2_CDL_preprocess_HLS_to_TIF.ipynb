{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "106a866f-39a2-4b20-8ba6-f5ec3d556561",
   "metadata": {},
   "outputs": [],
   "source": [
    "yoi = [2020]\n",
    "toi = ['15RTN', '15RTP', '14RNS']\n",
    "root_path = \"C:/Users/mcecil/CGA/CDL/\"\n",
    "spath = root_path + f\"CDL_HLS_dataframe{yoi[0]}.csv\"\n",
    "url_file = root_path + f'CDL_HLS_dataframe{yoi[0]}.csv'\n",
    "index_file = root_path + f'mask_index_{yoi[0]}.csv'\n",
    "\n",
    "image_index_file = root_path + f'image_index_{yoi[0]}.csv'\n",
    "\n",
    "kml_file = root_path + 'sentinel_tile_grid.kml'\n",
    "geojson_file = root_path + 'aoi.geojson'  ## chip file, lat-long\n",
    "geojson_rpj_file = root_path + 'aoi_rpj.geojson'\n",
    "hdf_dir = root_path + 'hdf/'\n",
    "tiff_dir = root_path + 'tif/'\n",
    "mask_dir = root_path + 'masks/'\n",
    "\n",
    "tile_src_path = root_path + \"tile_src.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "890a508e-f65d-4df7-84e9-172ba05b627d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Switch plotting backend\n",
    "import matplotlib\n",
    "matplotlib.use('agg')\n",
    "\n",
    "# Import required modules\n",
    "#import cartopy.crs as ccrs\n",
    "import os\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import geopandas\n",
    "#import pyhdf\n",
    "import rasterio\n",
    "import rasterio.mask\n",
    "import matplotlib.pyplot as pp\n",
    "import nasa_hls\n",
    "import numpy as np\n",
    "import time\n",
    "import fiona\n",
    "from pathlib import Path\n",
    "import time\n",
    "import urllib.request as urlreq\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94492d6e-242f-49aa-a8a8-ca1ccbbb3ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the HLS tiles and place there coordinates into a numpy array for processing later\n",
    "fiona.drvsupport.supported_drivers['KML'] = 'rw'\n",
    "tile_src = geopandas.read_file(kml_file, driver='KML')\n",
    "tile_name = []\n",
    "tile_x = []\n",
    "tile_y = []\n",
    "for tile_ind in range(tile_src.shape[0]):\n",
    "    tile_name.append(tile_src.iloc[tile_ind].Name)\n",
    "    tile_x.append(tile_src.iloc[tile_ind].geometry.centroid.x)\n",
    "    tile_y.append(tile_src.iloc[tile_ind].geometry.centroid.y)\n",
    "tile_name = np.array(tile_name)\n",
    "tile_x = np.array(tile_x)\n",
    "tile_y = np.array(tile_y)\n",
    "tile_src = pd.concat([tile_src, tile_src.bounds], axis = 1)\n",
    "#del tile_src\n",
    "tile_src.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae1ee1ab-d276-4bd8-ad3d-fe29d7182664",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the HLS query csv for this year\n",
    "qfn = open(url_file)\n",
    "qtile = []\n",
    "qyear = []\n",
    "qmonth = []\n",
    "qday = []\n",
    "qdate = []\n",
    "qurl = []\n",
    "for i, line in enumerate(qfn):\n",
    "    if (i == 0): # Skip header\n",
    "        continue\n",
    "    dummy = line.split(\",\")\n",
    "    qtile.append(dummy[2])\n",
    "    qdate.append(dummy[3])\n",
    "    qurl.append(dummy[4])\n",
    "    \n",
    "    qday.append(int(dummy[3].split(\"-\")[2]))\n",
    "    qmonth.append(int(dummy[3].split(\"-\")[1]))\n",
    "    qyear.append(int(dummy[3].split(\"-\")[0]))\n",
    "qfn.close()\n",
    "qdate = np.array(qdate)\n",
    "qtile = np.array(qtile)\n",
    "qurl = np.array(qurl)\n",
    "qmonth = np.array(qmonth)\n",
    "qyear = np.array(qyear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db83c256-6c7b-4f86-86e0-afcbe0c1e1f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the index file\n",
    "# fn_index = open(index_file, \"w\")\n",
    "# fn_index.write('EventID,Name,FireDate,MaskDate,BurnAcres,Tile,MaskFile')\n",
    "# fn_index.close()\n",
    "image_index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ecd3a8f-73b7-400c-a0ae-0cff713066d3",
   "metadata": {},
   "source": [
    "Load geojson used for query. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f600d0ca-467b-4ec2-a8f3-8f4b76434d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "aoi_src = geopandas.read_file(geojson_file) ## this could be a list\n",
    "nfeatures = aoi_src.shape[0]\n",
    "aoi_src = pd.concat([aoi_src, aoi_src.bounds], axis = 1)\n",
    "aoi_src['centroid_x'] = aoi_src.centroid.x\n",
    "aoi_src['centroid_y'] = aoi_src.centroid.y\n",
    "aoi_src['chip_id'] = range(1, len(aoi_src) + 1)\n",
    "aoi_src.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0319bc23-2006-4d35-b46e-d6d037a70f76",
   "metadata": {},
   "source": [
    "Loop through aoi_src (chips), associate closest tile, add ID field, download hdf (WORKING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d81f4f7d-d6b9-46b9-a4a5-f58f16b9700f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for aoi_ind in range(nfeatures):\n",
    "    aoi_row = aoi_src.iloc[aoi_ind]\n",
    "    print(aoi_row)\n",
    "    print(aoi_src.bounds)\n",
    "    aoi_chip_id = int(aoi_row.chip_id)\n",
    "    aoi_x = float(aoi_row.centroid_x)\n",
    "    aoi_y = float(aoi_row.centroid_y)\n",
    "        \n",
    "        \n",
    "    # Identify what tile the burn scar is in\n",
    "    s = (tile_x-aoi_x)**2+(tile_y-aoi_y)**2\n",
    "    tname = tile_name[np.argmin(s)]\n",
    "    print(s)\n",
    "    print(tname)\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    aoi_src.at[aoi_ind, 'tile'] = tname\n",
    "    aoi_src.at[aoi_ind, 'chip_id'] = 'chip_' + str(aoi_ind)\n",
    "    \n",
    "    \n",
    "    # Subset potential images based on tile and date\n",
    "    tile_mask = qtile == tname\n",
    "    # Subset based on date\n",
    "    date_mask1 = (qyear == yoi[0])\n",
    "  #  diff = (qmonth-bs_month)%12\n",
    "   # date_mask2 = ((diff <= month_threshold[1]) & (diff >= month_threshold[0]))\n",
    "    date_mask = date_mask1  #& date_mask2\n",
    "    \n",
    "    mask = tile_mask & date_mask\n",
    "    candidate_dates = qdate[mask]\n",
    "    candidate_tiles = qtile[mask]\n",
    "    candidate_urls = qurl[mask]\n",
    "    # Loop through images and check cloud cover\n",
    "    for cd, ct, cu in zip(candidate_dates, candidate_tiles, candidate_urls):     \n",
    "        local_name = cu.split('/')[-1].replace(\"\\n\", \"\")\n",
    "        # Download a candidate file to check cloud cover\n",
    "        try:\n",
    "            urlreq.urlretrieve(cu, filename=hdf_dir+local_name)\n",
    "        except:\n",
    "            continue\n",
    "#aoi_src.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "104f3200-7d45-4911-91ea-527a67e56f27",
   "metadata": {},
   "source": [
    "Extract cloud cover and spatial coverage from metadata (WORKING with adjusted function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4453f7d6-2dc9-4199-894f-9efbc09818c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from subprocess import Popen, PIPE\n",
    "\n",
    "def get_metadata_from_hdf_mine(src, fields=[\"cloud_cover\", \"spatial_coverage\"]):\n",
    "    \"\"\"Get metadata from a nasa-hls hdf file. See HLS user guide for valid fields.\n",
    "    \n",
    "    HLS User Guide - see Section 6.6: \n",
    "    \n",
    "    https://hls.gsfc.nasa.gov/wp-content/uploads/2019/01/HLS.v1.4.UserGuide_draft_ver3.1.pdf\n",
    "    \"\"\"\n",
    "    band=\"QA\"\n",
    "    cmd = f'gdalinfo HDF4_EOS:EOS_GRID:\"{src}\":Grid:{band}'\n",
    "    print('cmd')\n",
    "    print(cmd)\n",
    "    p = Popen(cmd, stdout=PIPE, shell=True)\n",
    "    output, err = p.communicate()\n",
    "    output = str(output)[2:-1].replace(\"\\\\n\", \"\\n\")\n",
    "    rc = p.returncode\n",
    "    metadata = {}\n",
    "    for line in output.split(\"\\n\"):\n",
    "        for field in fields:\n",
    "            if field in line:\n",
    "                metadata[field] = line.split(\"=\")[1].strip()\n",
    "                try:\n",
    "                    metadata[field] = float(metadata[field])\n",
    "                except:\n",
    "                    pass\n",
    "    for field in fields:\n",
    "        if field not in metadata.keys():\n",
    "            warnings.warn(f\"Could not find metadata for field '{field}'.\")\n",
    "    return metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4fa0074-2fca-42f6-8ed3-545a8e05e3f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_index = pd.DataFrame(columns = ['chip_id', 'image_id', 'tile', 'date', 'month', 'cloud_coverage', 'spatial_coverage'])\n",
    "\n",
    "\n",
    "for aoi_ind in range(nfeatures):\n",
    "    aoi_row = aoi_src.iloc[aoi_ind]\n",
    "    print(aoi_row)\n",
    "    print(aoi_src.bounds)\n",
    "    aoi_x = float(aoi_row.centroid_x)\n",
    "    aoi_y = float(aoi_row.centroid_y)\n",
    "\n",
    "        \n",
    "    # Identify what tile the burn scar is in\n",
    "    s = (tile_x-aoi_x)**2+(tile_y-aoi_y)**2\n",
    "    tname = tile_name[np.argmin(s)]\n",
    "    print(s)\n",
    "    print(tname)\n",
    "    aoi_chip_id = aoi_src.at[aoi_ind, 'chip_id']\n",
    "    print(aoi_chip_id)\n",
    "    aoi_src.at[aoi_ind, 'tile'] = tname\n",
    "   # aoi_src.at[aoi_ind, 'chip_id'] = 'chip_' + str(aoi_ind)\n",
    "    \n",
    "    # Subset potential images based on tile and date\n",
    "    tile_mask = qtile == tname\n",
    "    # Subset based on date\n",
    "    date_mask1 = (qyear == yoi[0])\n",
    "  #  diff = (qmonth-bs_month)%12\n",
    "   # date_mask2 = ((diff <= month_threshold[1]) & (diff >= month_threshold[0]))\n",
    "    date_mask = date_mask1  #& date_mask2\n",
    "    \n",
    "    mask = tile_mask & date_mask\n",
    "    candidate_dates = qdate[mask]\n",
    "    candidate_tiles = qtile[mask]\n",
    "    candidate_urls = qurl[mask]\n",
    "    # Loop through images and check cloud cover\n",
    "    for cd, ct, cu in zip(candidate_dates, candidate_tiles, candidate_urls):\n",
    "        print(cu)\n",
    "        local_name = cu.split('/')[-1].replace(\"\\n\", \"\")\n",
    "        \n",
    "        # if local_name != \"HLS.S30.T14RNS.2020117.v1.4.hdf\":\n",
    "        #     continue\n",
    "        # Download a candidate file to check cloud cover\n",
    "       # try:\n",
    "        print('md')\n",
    "        try:\n",
    "            md = get_metadata_from_hdf_mine(hdf_dir+local_name)\n",
    "        except:\n",
    "            print(cu + ' skipped')\n",
    "            continue\n",
    "        print(md)\n",
    "        cloud_cover = int(md['cloud_cover'].replace('\\\\r', ''))\n",
    "        print(cloud_cover)\n",
    "        spatial_coverage = int(md['spatial_coverage'].replace('\\\\r', ''))\n",
    "        print(spatial_coverage)\n",
    "        date = local_name.split('.')[3]\n",
    "        image_id = cu.replace('\\n', '').split('/')[-1].replace('.hdf', '')\n",
    "        #print(image_id)\n",
    "        image_date_string = image_id.split('.')[3]\n",
    "        image_date = pd.to_datetime(image_date_string, format=\"%Y%j\").date()\n",
    "        image_month = image_date.month\n",
    "         \n",
    "\n",
    "        new_row = pd.DataFrame({'chip_id':  [aoi_chip_id],\n",
    "                   'image_id':  [image_id],\n",
    "                   'tile': [tname],\n",
    "                   'date': [image_date],\n",
    "                   'month': [image_month],\n",
    "                   'cloud_coverage': [cloud_cover],\n",
    "                   'spatial_coverage': [spatial_coverage]})\n",
    "        image_index = pd.concat([image_index, new_row], ignore_index = True)\n",
    "\n",
    "\n",
    "#image_index.reset_index()\n",
    "#image_index.head(10)\n",
    "        # except:\n",
    "        #     print('error')\n",
    "        #     continue\n",
    "        \n",
    "#aoi_src.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd8e2252-4cf9-44ea-bb3c-84628f7d1f10",
   "metadata": {},
   "source": [
    "Convert HDF to single-layer TIF (Using HLS library, WORKING with adjusted function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08f3994c-c730-4e6d-a19c-0853be4a9cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import subprocess\n",
    "\n",
    "#from .utils import BAND_NAMES\n",
    "#from .utils import get_cloud_coverage_from_hdf\n",
    "BAND_NAMES = {'S30': {'Coastal_Aerosol': 'B01',\n",
    "                      'Blue': 'B02',\n",
    "                      'Green': 'B03',\n",
    "                      'Red': 'B04',\n",
    "                      'Red_Edge1': 'B05',\n",
    "                      'Red_Edge2': 'B06',\n",
    "                      'Red_Edge3': 'B07',\n",
    "                      'NIR_Broad': 'B08',\n",
    "                      'NIR_Narrow': 'B8A',\n",
    "                      'Water_Vapor' : 'B09',\n",
    "                      'Cirrus': 'B10',\n",
    "                      'SWIR1': 'B11',\n",
    "                      'SWIR2': 'B12',\n",
    "                      'QA': 'QA'},\n",
    "              'L30': {'Coastal_Aerosol': 'band01',\n",
    "                      'Blue': 'band02',\n",
    "                      'Green': 'band03',\n",
    "                      'Red': 'band04',\n",
    "                      'NIR': 'band05',\n",
    "                      'SWIR1': 'band06',\n",
    "                      'SWIR2': 'band07',\n",
    "                      'Cirrus': 'band09',\n",
    "                      'TIRS1': 'band10',\n",
    "                      'TIRS2': 'band11',\n",
    "                      'QA': 'QA'}}\n",
    "\n",
    "log = logging.getLogger(__name__)\n",
    "\n",
    "def convert_hdf2tiffs_mine(hdf_path, dstdir, bands=None, max_cloud_coverage=100,\n",
    "                      gdal_translate_options=None):\n",
    "    \"\"\"Convert (a subset of) hdf-file layers to single layer file GeoTiffs.\n",
    "    \n",
    "    Starting with GDAL 3.1 a Cloud Optimized GeoTIFF generator is available.\n",
    "    It can be used to generate COGs instead of normal GeoTIFFs.\n",
    "    For building a COG with default options simply use \n",
    "    `gdal_translate_options='-of COG'`.\n",
    "    For more information and options see https://gdal.org/drivers/raster/cog.html.\n",
    "    \"\"\"\n",
    "\n",
    "    if \".L30.\" in str(hdf_path):\n",
    "        product = \"L30\"\n",
    "    elif \".S30.\" in str(hdf_path):\n",
    "        product = \"S30\"\n",
    "    else:\n",
    "        log.exception(f\"FATAL ERROR : COULD NOT DERIVE PRODUCT.\")\n",
    "        raise ValueError(f\"Could not derive the product ('L30' or 'S30') from the {hdf_path}.\")\n",
    "\n",
    "    if bands is None:\n",
    "        bands = list(BAND_NAMES[product].keys())\n",
    "\n",
    "    return_none = False\n",
    "    for long_band_name in bands:\n",
    "        if long_band_name not in BAND_NAMES[product].keys():\n",
    "            continue\n",
    "        else:\n",
    "            band = BAND_NAMES[product][long_band_name]\n",
    "\n",
    "        if not isinstance(hdf_path, str):\n",
    "            hdf_path_str = str(hdf_path.resolve())\n",
    "        else:\n",
    "            hdf_path = Path(hdf_path)\n",
    "            hdf_path_str = hdf_path\n",
    "\n",
    "        if max_cloud_coverage < 100:\n",
    "            try:\n",
    "                cc = get_cloud_coverage_from_hdf(hdf_path_str)\n",
    "                log.debug(f\"DERIVED CLOUD COVER: {cc}\")\n",
    "            except:\n",
    "                log.exception(f\"COULD NOT DERIVE CLOUD COVER => PROCESSING ANYWAY: {hdf_path_str}\")\n",
    "                cc = 0\n",
    "\n",
    "            if cc > max_cloud_coverage:\n",
    "                log.debug(f\"SKIPPING CONVERSION - TOO HIGH CLOUD COVER: {cc}\")\n",
    "                return_none = True\n",
    "                break\n",
    "\n",
    "        dst = (dstdir / hdf_path.stem).resolve() / f\"{hdf_path.stem}__{long_band_name}.tif\"\n",
    "        if not dst.exists():\n",
    "            cmd = f\"gdal_translate HDF4_EOS:EOS_GRID:'{hdf_path_str}':Grid:{band} {dst}\"\n",
    "            if gdal_translate_options:\n",
    "                cmd += f\" {gdal_translate_options}\"\n",
    "            log.debug(f\"CMD: {cmd}\")\n",
    "            dst.parent.mkdir(exist_ok=True, parents=True)\n",
    "            try:\n",
    "                subprocess.check_call(cmd, shell=True)\n",
    "            except Exception:\n",
    "                log.exception(f\"ERROR DURING CONVERSION WITH CMD: {cmd}.\")\n",
    "    if return_none:\n",
    "        return None\n",
    "    else:\n",
    "        return dstdir / hdf_path.stem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9654ff4a-7955-4dbe-9978-8b6f61ac5d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for aoi_ind in range(nfeatures):\n",
    "    aoi_row = aoi_src.iloc[aoi_ind]\n",
    "    print(aoi_row)\n",
    "    print(aoi_src.bounds)\n",
    "    aoi_x = float(aoi_row.centroid_x)\n",
    "    aoi_y = float(aoi_row.centroid_y)\n",
    "        \n",
    "    # Identify what tile the burn scar is in\n",
    "    s = (tile_x-aoi_x)**2+(tile_y-aoi_y)**2\n",
    "    tname = tile_name[np.argmin(s)]\n",
    "    print(s)\n",
    "    print(tname)\n",
    "    aoi_src.at[aoi_ind, 'tile'] = tname\n",
    "    aoi_src.at[aoi_ind, 'chip_id'] = 'chip_' + str(aoi_ind)\n",
    "    \n",
    "    # Subset potential images based on tile and date\n",
    "    tile_mask = qtile == tname\n",
    "    # Subset based on date\n",
    "    date_mask1 = (qyear == yoi[0])\n",
    "  #  diff = (qmonth-bs_month)%12\n",
    "   # date_mask2 = ((diff <= month_threshold[1]) & (diff >= month_threshold[0]))\n",
    "    date_mask = date_mask1  #& date_mask2\n",
    "    \n",
    "    mask = tile_mask & date_mask\n",
    "    candidate_dates = qdate[mask]\n",
    "    candidate_tiles = qtile[mask]\n",
    "    candidate_urls = qurl[mask]\n",
    "    # Loop through images and check cloud cover\n",
    "    for cd, ct, cu in zip(candidate_dates, candidate_tiles, candidate_urls):\n",
    "        print(cu)\n",
    "        local_name = cu.split('/')[-1].replace(\"\\n\", \"\")\n",
    "        \n",
    "        if local_name != \"HLS.S30.T14RNS.2020117.v1.4.hdf\":\n",
    "            continue\n",
    "        print('test')\n",
    "        print(Path(hdf_dir+local_name))\n",
    "        print(Path(tiff_dir))\n",
    "        convert_hdf2tiffs_mine(Path(hdf_dir+local_name), Path(tiff_dir))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64823ba5-0b08-4038-8ab2-d3419e42c3c1",
   "metadata": {},
   "source": [
    "Create mask tif files (1 if valid data, 0 if not). Uses geojson (lat long) to mask. Does not crop. (WORKS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e9c9262-9312-448e-8d84-d1cd5ca540e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for aoi_ind in range(nfeatures):\n",
    "    aoi_row = aoi_src.iloc[aoi_ind]\n",
    "    print(aoi_row)\n",
    "    print(aoi_src.bounds)\n",
    "    aoi_x = float(aoi_row.centroid_x)\n",
    "    aoi_y = float(aoi_row.centroid_y)\n",
    "        \n",
    "    # Identify what tile the burn scar is in\n",
    "    s = (tile_x-aoi_x)**2+(tile_y-aoi_y)**2\n",
    "    tname = tile_name[np.argmin(s)]\n",
    "    print(s)\n",
    "    print(tname)\n",
    "    aoi_src.at[aoi_ind, 'tile'] = tname\n",
    "    aoi_src.at[aoi_ind, 'chip_id'] = 'chip_' + str(aoi_ind)\n",
    "    \n",
    "    # Subset potential images based on tile and date\n",
    "    tile_mask = qtile == tname\n",
    "    # Subset based on date\n",
    "    date_mask1 = (qyear == yoi[0])\n",
    "  #  diff = (qmonth-aoi_month)%12\n",
    "   # date_mask2 = ((diff <= month_threshold[1]) & (diff >= month_threshold[0]))\n",
    "    date_mask = date_mask1  #& date_mask2\n",
    "    \n",
    "    mask = tile_mask & date_mask\n",
    "    candidate_dates = qdate[mask]\n",
    "    candidate_tiles = qtile[mask]\n",
    "    candidate_urls = qurl[mask]\n",
    "    # Loop through images and check cloud cover\n",
    "    for cd, ct, cu in zip(candidate_dates, candidate_tiles, candidate_urls):\n",
    "       # print(cu)\n",
    "        local_name = cu.split('/')[-1].replace(\"\\n\", \"\")\n",
    "        print(local_name)\n",
    "        # if local_name != \"HLS.S30.T14RNS.2020345.v1.4.hdf\":\n",
    "        #     continue\n",
    "        # print('valid')\n",
    "        # Download a candidate file to check cloud cover\n",
    "        # try:\n",
    "        #     urlreq.urlretrieve(cu, filename=hdf_dir+local_name)\n",
    "        # except:\n",
    "        #     continue\n",
    "    \n",
    "        # Check cloud cover and spacial coverage\n",
    "      #  try:\n",
    "            # md = nasa_hls.get_metadata_from_hdf(hdf_dir+local_name, fields=['cloud_cover', 'spatial_coverage'])\n",
    "            # if ((md[\"cloud_cover\"] < cldfrac_threshold) and (md['spatial_coverage'] > spacecov_threshold)):\n",
    "            \n",
    "                # Convert to a geotiff            \n",
    "                # nasa_hls.convert_hdf2tiffs(Path(hdf_dir+local_name), Path(tiff_dir))\n",
    "\n",
    "            # Open the new geotiff\n",
    "        date = datetime.strptime(cd, '%Y-%m-%d')\n",
    "        doy = (date-datetime(date.year, 1, 1)).days+1\n",
    "        fp = f'HLS.S30.T{ct}.{date.year}{doy:03d}.v1.4'\n",
    "        src = rasterio.open(f'{tiff_dir}/{fp}__B04.tif')\n",
    "        red = src.read(1)\n",
    "        print(src)\n",
    "        print(red)\n",
    "\n",
    "        # Open NIR too to compute NDVI\n",
    "        nir_src = rasterio.open(f'{tiff_dir}/{fp}__B08.tif')\n",
    "        nir = nir_src.read(1)\n",
    "\n",
    "        \n",
    "        qa_src = rasterio.open(f'{tiff_dir}/{fp}__QA.tif')\n",
    "        qa = qa_src.read(1)\n",
    "        \n",
    "        print(qa_src)\n",
    "        print(qa)\n",
    "        \n",
    "        # Compute NDVI\n",
    "        red = np.clip(red/15000, 0, 1)\n",
    "        nir = np.clip(nir/15000, 0, 1)\n",
    "        ndvi = np.clip((nir-red)/(nir+red), 0, 1)\n",
    "\n",
    "        # Convert burn scar shape to proper geometry\n",
    "        aoi_shape = aoi_src.to_crs(src.crs).iloc[aoi_ind]\n",
    "\n",
    "        print(aoi_shape)\n",
    "        # Build the mask\n",
    "        out_image, out_transform = rasterio.mask.mask(src, [aoi_shape.geometry], crop=False)\n",
    "        print(np.min(out_image), np.max(out_image))\n",
    "        out_image = np.clip(out_image, 0, 1)\n",
    "        print(np.min(out_image), np.max(out_image))\n",
    "\n",
    "        out_meta = src.meta\n",
    "        out_meta.update({'driver':'GTiff', 'height':out_image.shape[1],\n",
    "            'width':out_image.shape[2], 'transform':out_transform})\n",
    "\n",
    "        with rasterio.open(f'{mask_dir}/{fp}.mask.tif', 'w', **out_meta) as dest:\n",
    "            dest.write(out_image)\n",
    "\n",
    "        # Plot the original and mask\n",
    "        fig, [ax1, ax2] = pp.subplots(ncols=2)\n",
    "        ax1.imshow(ndvi, cmap='YlGn', vmin=0, vmax=1)\n",
    "        ax2.imshow(np.squeeze(out_image), cmap='Greys')\n",
    "        pp.savefig(f'{mask_dir}/{fp}.mask.jpg')\n",
    "        pp.close()\n",
    "\n",
    "        # Save in index file\n",
    "#         fn_index = open(index_file, 'a')\n",
    "#         fn_index.write(f'\\n{aoi_row.Event_ID},{aoi_row.Incid_Name},{aoi_row.Ig_Date},{cd},{aoi_row.BurnBndAc},{ct},{fp}.mask.tif')\n",
    "#         fn_index.close()\n",
    "\n",
    "#         # Move onto the next burn scar\n",
    "#         del src\n",
    "#         del nir_src\n",
    "       # break\n",
    "        # except:\n",
    "        #     print('error')\n",
    "        #     continue"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
