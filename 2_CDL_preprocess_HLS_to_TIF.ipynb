{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "106a866f-39a2-4b20-8ba6-f5ec3d556561",
   "metadata": {},
   "outputs": [],
   "source": [
    "yoi = [2020]\n",
    "toi = ['15RTN', '15RTP', '14RNS']\n",
    "root_path = \"C:/Users/mcecil/CGA/CDL/\"\n",
    "spath = root_path + f\"CDL_HLS_dataframe{yoi[0]}.csv\"\n",
    "url_file = root_path + f'CDL_HLS_dataframe{yoi[0]}.csv'\n",
    "index_file = root_path + f'mask_index_{yoi[0]}.csv'\n",
    "\n",
    "image_index_file = root_path + f'image_index_{yoi[0]}.csv'\n",
    "\n",
    "kml_file = root_path + 'sentinel_tile_grid.kml'\n",
    "geojson_file = root_path + 'geojson/' + 'aoi.geojson'  ## chip file, lat-long\n",
    "geojson_rpj_file = root_path + 'aoi_rpj.geojson'\n",
    "hdf_dir = root_path + 'hdf/'\n",
    "tiff_dir = root_path + 'tif/'\n",
    "tiff_dir_rpj = root_path + 'tif_5070_new/'\n",
    "\n",
    "mask_dir = root_path + 'masks/'\n",
    "\n",
    "tile_src_path = root_path + \"tile_src.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "890a508e-f65d-4df7-84e9-172ba05b627d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Switch plotting backend\n",
    "import matplotlib\n",
    "matplotlib.use('agg')\n",
    "\n",
    "# Import required modules\n",
    "#import cartopy.crs as ccrs\n",
    "import os\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import geopandas\n",
    "#import pyhdf\n",
    "import rasterio\n",
    "import rasterio.mask\n",
    "import matplotlib.pyplot as pp\n",
    "import nasa_hls\n",
    "import numpy as np\n",
    "import time\n",
    "import fiona\n",
    "from pathlib import Path\n",
    "import time\n",
    "import urllib.request as urlreq\n",
    "import shutil\n",
    "from glob import glob\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94492d6e-242f-49aa-a8a8-ca1ccbbb3ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load the HLS tiles and place there coordinates into a numpy array for processing later\n",
    "# fiona.drvsupport.supported_drivers['KML'] = 'rw'\n",
    "# tile_src = geopandas.read_file(kml_file, driver='KML')\n",
    "# tile_name = []\n",
    "# tile_x = []\n",
    "# tile_y = []\n",
    "# for tile_ind in range(tile_src.shape[0]):\n",
    "#     tile_name.append(tile_src.iloc[tile_ind].Name)\n",
    "#     tile_x.append(tile_src.iloc[tile_ind].geometry.centroid.x)\n",
    "#     tile_y.append(tile_src.iloc[tile_ind].geometry.centroid.y)\n",
    "# tile_name = np.array(tile_name)\n",
    "# tile_x = np.array(tile_x)\n",
    "# tile_y = np.array(tile_y)\n",
    "# tile_src = pd.concat([tile_src, tile_src.bounds], axis = 1)\n",
    "# #del tile_src\n",
    "# tile_src.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae1ee1ab-d276-4bd8-ad3d-fe29d7182664",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load the HLS query csv for this year\n",
    "# qfn = open(url_file)\n",
    "# qtile = []\n",
    "# qyear = []\n",
    "# qmonth = []\n",
    "# qday = []\n",
    "# qdate = []\n",
    "# qurl = []\n",
    "# for i, line in enumerate(qfn):\n",
    "#     if (i == 0): # Skip header\n",
    "#         continue\n",
    "#     dummy = line.split(\",\")\n",
    "#     qtile.append(dummy[2])\n",
    "#     qdate.append(dummy[3])\n",
    "#     qurl.append(dummy[4])\n",
    "    \n",
    "#     qday.append(int(dummy[3].split(\"-\")[2]))\n",
    "#     qmonth.append(int(dummy[3].split(\"-\")[1]))\n",
    "#     qyear.append(int(dummy[3].split(\"-\")[0]))\n",
    "# qfn.close()\n",
    "# qdate = np.array(qdate)\n",
    "# qtile = np.array(qtile)\n",
    "# qurl = np.array(qurl)\n",
    "# qmonth = np.array(qmonth)\n",
    "# qyear = np.array(qyear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db83c256-6c7b-4f86-86e0-afcbe0c1e1f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the index file\n",
    "# fn_index = open(index_file, \"w\")\n",
    "# fn_index.write('EventID,Name,FireDate,MaskDate,BurnAcres,Tile,MaskFile')\n",
    "# fn_index.close()\n",
    "#image_index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ecd3a8f-73b7-400c-a0ae-0cff713066d3",
   "metadata": {},
   "source": [
    "Load geojson used for query. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f600d0ca-467b-4ec2-a8f3-8f4b76434d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "aoi_src = geopandas.read_file(geojson_file) ## this could be a list\n",
    "nfeatures = aoi_src.shape[0]\n",
    "aoi_src = pd.concat([aoi_src, aoi_src.bounds], axis = 1)\n",
    "aoi_src['centroid_x'] = aoi_src.centroid.x\n",
    "aoi_src['centroid_y'] = aoi_src.centroid.y\n",
    "aoi_src['chip_id'] = range(1, len(aoi_src) + 1)\n",
    "aoi_src.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0319bc23-2006-4d35-b46e-d6d037a70f76",
   "metadata": {},
   "source": [
    "Download HDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d81f4f7d-d6b9-46b9-a4a5-f58f16b9700f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for aoi_ind in range(nfeatures):\n",
    "    aoi_row = aoi_src.iloc[aoi_ind]\n",
    "    print(aoi_row)\n",
    "    print(aoi_src.bounds)\n",
    "    aoi_chip_id = int(aoi_row.chip_id)\n",
    "    aoi_x = float(aoi_row.centroid_x)\n",
    "    aoi_y = float(aoi_row.centroid_y)\n",
    "        \n",
    "        \n",
    "    # Identify what tile the burn scar is in\n",
    "    s = (tile_x-aoi_x)**2+(tile_y-aoi_y)**2\n",
    "    tname = tile_name[np.argmin(s)]\n",
    "    print(s)\n",
    "    print(tname)\n",
    "    \n",
    "    aoi_src.at[aoi_ind, 'tile'] = tname\n",
    "   # aoi_src.at[aoi_ind, 'chip_id'] = 'chip_' + str(aoi_chip_id)\n",
    "    \n",
    "    \n",
    "    # Subset potential images based on tile and date\n",
    "    tile_mask = qtile == tname\n",
    "    # Subset based on date\n",
    "    date_mask1 = (qyear == yoi[0])\n",
    "  #  diff = (qmonth-bs_month)%12\n",
    "   # date_mask2 = ((diff <= month_threshold[1]) & (diff >= month_threshold[0]))\n",
    "    date_mask = date_mask1  #& date_mask2\n",
    "    \n",
    "    mask = tile_mask & date_mask\n",
    "    candidate_dates = qdate[mask]\n",
    "    candidate_tiles = qtile[mask]\n",
    "    candidate_urls = qurl[mask]\n",
    "    # Loop through images and check cloud cover\n",
    "    for cd, ct, cu in zip(candidate_dates, candidate_tiles, candidate_urls):     \n",
    "        local_name = cu.split('/')[-1].replace(\"\\n\", \"\")\n",
    "        print(local_name)\n",
    "        if (local_name != 'HLS.S30.T14RNS.2020040.v1.4.hdf'):\n",
    "            continue\n",
    "        # Download a candidate file to check cloud cover\n",
    "        try:\n",
    "            urlreq.urlretrieve(cu, filename=hdf_dir+local_name)\n",
    "        except:\n",
    "            continue\n",
    "#aoi_src.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "104f3200-7d45-4911-91ea-527a67e56f27",
   "metadata": {},
   "source": [
    "Extract metadata (WORKING with adjusted function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4453f7d6-2dc9-4199-894f-9efbc09818c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from subprocess import Popen, PIPE\n",
    "\n",
    "def get_metadata_from_hdf_mine(src, fields=[\"cloud_cover\", \"spatial_coverage\"]):\n",
    "    \"\"\"Get metadata from a nasa-hls hdf file. See HLS user guide for valid fields.\n",
    "    \n",
    "    HLS User Guide - see Section 6.6: \n",
    "    \n",
    "    https://hls.gsfc.nasa.gov/wp-content/uploads/2019/01/HLS.v1.4.UserGuide_draft_ver3.1.pdf\n",
    "    \"\"\"\n",
    "    band=\"QA\"\n",
    "    cmd = f'gdalinfo HDF4_EOS:EOS_GRID:\"{src}\":Grid:{band}'\n",
    "    print('cmd')\n",
    "    print(cmd)\n",
    "    p = Popen(cmd, stdout=PIPE, shell=True)\n",
    "    output, err = p.communicate()\n",
    "    output = str(output)[2:-1].replace(\"\\\\n\", \"\\n\")\n",
    "    rc = p.returncode\n",
    "    metadata = {}\n",
    "    for line in output.split(\"\\n\"):\n",
    "        for field in fields:\n",
    "            if field in line:\n",
    "                metadata[field] = line.split(\"=\")[1].strip()\n",
    "                try:\n",
    "                    metadata[field] = float(metadata[field])\n",
    "                except:\n",
    "                    pass\n",
    "    for field in fields:\n",
    "        if field not in metadata.keys():\n",
    "            warnings.warn(f\"Could not find metadata for field '{field}'.\")\n",
    "    return metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2a6246b-fb71-4f0f-aa24-dc7c438274f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "candidate_hdf = sorted(glob(hdf_dir + '*.hdf'))\n",
    "#print(candidate_hdf)\n",
    "for name in candidate_hdf:\n",
    "    local_name = name.replace('\\\\', '/').split('/')[-1].replace(\"\\n\", \"\")\n",
    "    print(local_name)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4fa0074-2fca-42f6-8ed3-545a8e05e3f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_index = pd.DataFrame(columns = ['image_id', 'tile', 'date', 'month', 'cloud_coverage', 'spatial_coverage'])\n",
    "\n",
    "candidate_hdf = sorted(glob(hdf_dir + '*.hdf'))\n",
    "\n",
    "for img in candidate_hdf:\n",
    "    print(img)\n",
    "    \n",
    "  \n",
    "    local_name = img.replace('\\\\', '/').split('/')[-1].replace(\"\\n\", \"\")\n",
    "    print(local_name)\n",
    "    # if local_name != \"HLS.S30.T14RNS.2020117.v1.4.hdf\":\n",
    "    #     continue\n",
    "    # Download a candidate file to check cloud cover\n",
    "   # try:\n",
    "  #  print('md')\n",
    "    try:\n",
    "        print(hdf_dir+local_name)\n",
    "        md = get_metadata_from_hdf_mine(hdf_dir+local_name)\n",
    "    except:\n",
    "        print(img + ' skipped')\n",
    "        continue\n",
    "    print(md)\n",
    "    cloud_cover = int(md['cloud_cover'].replace('\\\\r', ''))\n",
    "    print(cloud_cover)\n",
    "    spatial_coverage = int(md['spatial_coverage'].replace('\\\\r', ''))\n",
    "    print(spatial_coverage)\n",
    "    image_id = local_name.replace('.hdf', '')\n",
    "    date = local_name.split('.')[3]\n",
    "    image_date_string = image_id.split('.')[3]\n",
    "    image_date = pd.to_datetime(image_date_string, format=\"%Y%j\").date()\n",
    "    image_month = image_date.month\n",
    "\n",
    "\n",
    "    new_row = pd.DataFrame({'image_id':  [image_id],\n",
    "               'tile': [tname],\n",
    "               'date': [image_date],\n",
    "               'month': [image_month],\n",
    "               'cloud_coverage': [cloud_cover],\n",
    "               'spatial_coverage': [spatial_coverage]})\n",
    "    image_index = pd.concat([image_index, new_row], ignore_index = True)\n",
    "\n",
    "\n",
    "#image_index.reset_index()\n",
    "        # except:\n",
    "        #     print('error')\n",
    "        #     continue\n",
    "        \n",
    "#aoi_src.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb673ec-154b-4a3c-9823-3a253b91aa4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_index.head(10)\n",
    "#image_index.to_csv(image_index_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd8e2252-4cf9-44ea-bb3c-84628f7d1f10",
   "metadata": {},
   "source": [
    "Convert HDF to single-layer TIF (Using HLS library, WORKING with adjusted function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "08f3994c-c730-4e6d-a19c-0853be4a9cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import subprocess\n",
    "\n",
    "#from .utils import BAND_NAMES\n",
    "#from .utils import get_cloud_coverage_from_hdf\n",
    "BAND_NAMES = {'S30': {'Coastal_Aerosol': 'B01',\n",
    "                      'Blue': 'B02',\n",
    "                      'Green': 'B03',\n",
    "                      'Red': 'B04',\n",
    "                      'Red_Edge1': 'B05',\n",
    "                      'Red_Edge2': 'B06',\n",
    "                      'Red_Edge3': 'B07',\n",
    "                      'NIR_Broad': 'B08',\n",
    "                      'NIR_Narrow': 'B8A',\n",
    "                      'Water_Vapor' : 'B09',\n",
    "                      'Cirrus': 'B10',\n",
    "                      'SWIR1': 'B11',\n",
    "                      'SWIR2': 'B12',\n",
    "                      'QA': 'QA'},\n",
    "              'L30': {'Coastal_Aerosol': 'band01',\n",
    "                      'Blue': 'band02',\n",
    "                      'Green': 'band03',\n",
    "                      'Red': 'band04',\n",
    "                      'NIR': 'band05',\n",
    "                      'SWIR1': 'band06',\n",
    "                      'SWIR2': 'band07',\n",
    "                      'Cirrus': 'band09',\n",
    "                      'TIRS1': 'band10',\n",
    "                      'TIRS2': 'band11',\n",
    "                      'QA': 'QA'}}\n",
    "\n",
    "log = logging.getLogger(__name__)\n",
    "\n",
    "def convert_hdf2tiffs_mine(hdf_path, dstdir, dstdir_rpj, bands=None, max_cloud_coverage=100,\n",
    "                      gdal_translate_options=None, delete_original_tiff = False):\n",
    "    \"\"\"Convert (a subset of) hdf-file layers to single layer file GeoTiffs.\n",
    "    \n",
    "    Starting with GDAL 3.1 a Cloud Optimized GeoTIFF generator is available.\n",
    "    It can be used to generate COGs instead of normal GeoTIFFs.\n",
    "    For building a COG with default options simply use \n",
    "    `gdal_translate_options='-of COG'`.\n",
    "    For more information and options see https://gdal.org/drivers/raster/cog.html.\n",
    "    \"\"\"\n",
    "\n",
    "    if \".L30.\" in str(hdf_path):\n",
    "        product = \"L30\"\n",
    "    elif \".S30.\" in str(hdf_path):\n",
    "        product = \"S30\"\n",
    "    else:\n",
    "        log.exception(f\"FATAL ERROR : COULD NOT DERIVE PRODUCT.\")\n",
    "        raise ValueError(f\"Could not derive the product ('L30' or 'S30') from the {hdf_path}.\")\n",
    "\n",
    "    if bands is None:\n",
    "        bands = list(BAND_NAMES[product].keys())\n",
    "\n",
    "    return_none = False\n",
    "    for long_band_name in bands:\n",
    "        if long_band_name not in BAND_NAMES[product].keys():\n",
    "            continue\n",
    "        else:\n",
    "            band = BAND_NAMES[product][long_band_name]\n",
    "\n",
    "        if not isinstance(hdf_path, str):\n",
    "            hdf_path_str = str(hdf_path.resolve())\n",
    "        else:\n",
    "            hdf_path = Path(hdf_path)\n",
    "            hdf_path_str = hdf_path\n",
    "\n",
    "        if max_cloud_coverage < 100:\n",
    "            try:\n",
    "                cc = get_cloud_coverage_from_hdf(hdf_path_str)\n",
    "                log.debug(f\"DERIVED CLOUD COVER: {cc}\")\n",
    "            except:\n",
    "                log.exception(f\"COULD NOT DERIVE CLOUD COVER => PROCESSING ANYWAY: {hdf_path_str}\")\n",
    "                cc = 0\n",
    "\n",
    "            if cc > max_cloud_coverage:\n",
    "                log.debug(f\"SKIPPING CONVERSION - TOO HIGH CLOUD COVER: {cc}\")\n",
    "                return_none = True\n",
    "                break\n",
    "\n",
    "        dst = (dstdir / hdf_path.stem).resolve() / f\"{hdf_path.stem}__{long_band_name}.tif\"\n",
    "        dst_rpj = (dstdir_rpj / hdf_path.stem).resolve() / f\"{hdf_path.stem}__{long_band_name}__5070.tif\"\n",
    "      #  print('test2')\n",
    "      #  print(dst)\n",
    "      #  print(dst_rpj)\n",
    "\n",
    "        if not dst.exists():\n",
    "            print('creating files')\n",
    "            cmd = f'gdal_translate HDF4_EOS:EOS_GRID:\"{hdf_path_str}\":Grid:{band} {dst}'\n",
    "            if gdal_translate_options:\n",
    "                cmd += f\" {gdal_translate_options}\"\n",
    "            log.debug(f\"CMD: {cmd}\")\n",
    "            dst.parent.mkdir(exist_ok=True, parents=True)\n",
    "            try:\n",
    "                subprocess.check_call(cmd, shell=True)\n",
    "            except Exception:\n",
    "                log.exception(f\"ERROR DURING CONVERSION WITH CMD: {cmd}.\")\n",
    "            ## reprojection\n",
    "          #  gdalwarp -t_srs EPSG:5070 -r bilinear -tr 30 -30 <input_file> <output_file>\n",
    "            cmd = f\"gdalwarp -t_srs EPSG:5070 -r bilinear -tr 30 -30 {dst} {dst_rpj}\"\n",
    "          #  if gdal_translate_options:\n",
    "           #     cmd += f\" {gdal_translate_options}\"\n",
    "            log.debug(f\"CMD: {cmd}\")\n",
    "            dst_rpj.parent.mkdir(exist_ok=True, parents=True)\n",
    "            try:\n",
    "                subprocess.check_call(cmd, shell=True)\n",
    "            except Exception:\n",
    "                log.exception(f\"ERROR DURING REPROJECTION WITH CMD: {cmd}.\")   \n",
    "    \n",
    "    if delete_original_tiff:\n",
    "        shutil.rmtree((dstdir / hdf_path.stem).resolve())\n",
    "                \n",
    "            \n",
    "    if return_none:\n",
    "        return None\n",
    "    else:\n",
    "        return dstdir / hdf_path.stem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "85287390-6f10-428b-a57e-8d67ebe64f6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>image_id</th>\n",
       "      <th>tile</th>\n",
       "      <th>date</th>\n",
       "      <th>month</th>\n",
       "      <th>cloud_coverage</th>\n",
       "      <th>spatial_coverage</th>\n",
       "      <th>convert_to_tif</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>HLS.S30.T14RNS.2020002.v1.4</td>\n",
       "      <td>14RNS</td>\n",
       "      <td>2020-01-02</td>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>41</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>HLS.S30.T14RNS.2020005.v1.4</td>\n",
       "      <td>14RNS</td>\n",
       "      <td>2020-01-05</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>100</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>HLS.S30.T14RNS.2020007.v1.4</td>\n",
       "      <td>14RNS</td>\n",
       "      <td>2020-01-07</td>\n",
       "      <td>1</td>\n",
       "      <td>29</td>\n",
       "      <td>41</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>HLS.S30.T14RNS.2020010.v1.4</td>\n",
       "      <td>14RNS</td>\n",
       "      <td>2020-01-10</td>\n",
       "      <td>1</td>\n",
       "      <td>89</td>\n",
       "      <td>100</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>HLS.S30.T14RNS.2020012.v1.4</td>\n",
       "      <td>14RNS</td>\n",
       "      <td>2020-01-12</td>\n",
       "      <td>1</td>\n",
       "      <td>71</td>\n",
       "      <td>41</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                     image_id   tile        date  month   \n",
       "0           0  HLS.S30.T14RNS.2020002.v1.4  14RNS  2020-01-02      1  \\\n",
       "1           1  HLS.S30.T14RNS.2020005.v1.4  14RNS  2020-01-05      1   \n",
       "2           2  HLS.S30.T14RNS.2020007.v1.4  14RNS  2020-01-07      1   \n",
       "3           3  HLS.S30.T14RNS.2020010.v1.4  14RNS  2020-01-10      1   \n",
       "4           4  HLS.S30.T14RNS.2020012.v1.4  14RNS  2020-01-12      1   \n",
       "\n",
       "   cloud_coverage  spatial_coverage  convert_to_tif  \n",
       "0             100                41           False  \n",
       "1               0               100           False  \n",
       "2              29                41           False  \n",
       "3              89               100           False  \n",
       "4              71                41           False  "
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_index = pd.read_csv(image_index_file)\n",
    "image_index.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "9654ff4a-7955-4dbe-9978-8b6f61ac5d1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HLS.S30.T14RNS.2020002.v1.4\n",
      "invalid\n",
      "HLS.S30.T14RNS.2020005.v1.4\n",
      "invalid\n",
      "HLS.S30.T14RNS.2020007.v1.4\n",
      "invalid\n",
      "HLS.S30.T14RNS.2020010.v1.4\n",
      "invalid\n",
      "HLS.S30.T14RNS.2020012.v1.4\n",
      "invalid\n",
      "HLS.S30.T14RNS.2020015.v1.4\n",
      "invalid\n",
      "HLS.S30.T14RNS.2020017.v1.4\n",
      "invalid\n",
      "HLS.S30.T14RNS.2020040.v1.4\n",
      "invalid\n",
      "HLS.S30.T14RNS.2020042.v1.4\n",
      "invalid\n",
      "HLS.S30.T14RNS.2020047.v1.4\n",
      "invalid\n",
      "HLS.S30.T14RNS.2020057.v1.4\n",
      "invalid\n",
      "HLS.S30.T14RNS.2020067.v1.4\n",
      "invalid\n",
      "HLS.S30.T14RNS.2020070.v1.4\n",
      "invalid\n",
      "HLS.S30.T14RNS.2020072.v1.4\n",
      "invalid\n",
      "HLS.S30.T14RNS.2020075.v1.4\n",
      "invalid\n",
      "HLS.S30.T14RNS.2020077.v1.4\n",
      "invalid\n",
      "HLS.S30.T14RNS.2020080.v1.4\n",
      "invalid\n",
      "HLS.S30.T14RNS.2020082.v1.4\n",
      "invalid\n",
      "HLS.S30.T14RNS.2020087.v1.4\n",
      "invalid\n",
      "HLS.S30.T14RNS.2020097.v1.4\n",
      "invalid\n",
      "HLS.S30.T14RNS.2020100.v1.4\n",
      "invalid\n",
      "HLS.S30.T14RNS.2020102.v1.4\n",
      "invalid\n",
      "HLS.S30.T14RNS.2020107.v1.4\n",
      "valid\n",
      "HLS.S30.T14RNS.2020107.v1.4\n",
      "creating files\n",
      "creating files\n",
      "creating files\n",
      "creating files\n",
      "creating files\n",
      "creating files\n",
      "creating files\n",
      "creating files\n",
      "creating files\n",
      "creating files\n",
      "creating files\n",
      "creating files\n",
      "creating files\n",
      "creating files\n",
      "HLS.S30.T14RNS.2020110.v1.4\n",
      "valid\n",
      "HLS.S30.T14RNS.2020110.v1.4\n",
      "creating files\n",
      "creating files\n",
      "creating files\n",
      "creating files\n",
      "creating files\n",
      "creating files\n",
      "creating files\n",
      "creating files\n",
      "creating files\n",
      "creating files\n",
      "creating files\n",
      "creating files\n",
      "creating files\n",
      "creating files\n",
      "HLS.S30.T14RNS.2020112.v1.4\n",
      "invalid\n",
      "HLS.S30.T14RNS.2020115.v1.4\n",
      "valid\n",
      "HLS.S30.T14RNS.2020115.v1.4\n",
      "creating files\n",
      "creating files\n",
      "creating files\n",
      "creating files\n",
      "creating files\n",
      "creating files\n",
      "creating files\n",
      "creating files\n",
      "creating files\n",
      "creating files\n",
      "creating files\n",
      "creating files\n",
      "creating files\n",
      "creating files\n",
      "HLS.S30.T14RNS.2020117.v1.4\n",
      "valid\n",
      "HLS.S30.T14RNS.2020117.v1.4\n",
      "creating files\n",
      "creating files\n",
      "creating files\n",
      "creating files\n",
      "creating files\n",
      "creating files\n",
      "creating files\n",
      "creating files\n",
      "creating files\n",
      "creating files\n",
      "creating files\n",
      "creating files\n",
      "creating files\n",
      "creating files\n",
      "HLS.S30.T14RNS.2020120.v1.4\n",
      "invalid\n",
      "HLS.S30.T14RNS.2020122.v1.4\n",
      "valid\n",
      "HLS.S30.T14RNS.2020122.v1.4\n",
      "creating files\n",
      "creating files\n",
      "creating files\n",
      "creating files\n",
      "creating files\n",
      "creating files\n",
      "creating files\n",
      "creating files\n",
      "creating files\n",
      "creating files\n",
      "creating files\n",
      "creating files\n",
      "creating files\n",
      "creating files\n",
      "HLS.S30.T14RNS.2020125.v1.4\n",
      "invalid\n",
      "HLS.S30.T14RNS.2020127.v1.4\n",
      "valid\n",
      "HLS.S30.T14RNS.2020127.v1.4\n",
      "creating files\n",
      "creating files\n",
      "creating files\n",
      "creating files\n",
      "creating files\n",
      "creating files\n",
      "creating files\n",
      "creating files\n",
      "creating files\n",
      "creating files\n",
      "creating files\n",
      "creating files\n",
      "creating files\n",
      "creating files\n",
      "HLS.S30.T14RNS.2020130.v1.4\n",
      "valid\n",
      "HLS.S30.T14RNS.2020130.v1.4\n",
      "creating files\n",
      "creating files\n",
      "creating files\n",
      "creating files\n",
      "creating files\n",
      "creating files\n",
      "creating files\n",
      "creating files\n",
      "creating files\n",
      "creating files\n",
      "creating files\n",
      "creating files\n",
      "creating files\n",
      "creating files\n",
      "HLS.S30.T14RNS.2020132.v1.4\n",
      "invalid\n",
      "HLS.S30.T14RNS.2020135.v1.4\n",
      "valid\n",
      "HLS.S30.T14RNS.2020135.v1.4\n",
      "creating files\n",
      "creating files\n",
      "creating files\n",
      "creating files\n",
      "creating files\n",
      "creating files\n",
      "creating files\n",
      "creating files\n",
      "creating files\n",
      "creating files\n",
      "creating files\n",
      "creating files\n",
      "creating files\n",
      "creating files\n",
      "HLS.S30.T14RNS.2020137.v1.4\n",
      "invalid\n",
      "HLS.S30.T14RNS.2020140.v1.4\n",
      "valid\n",
      "HLS.S30.T14RNS.2020140.v1.4\n",
      "creating files\n",
      "creating files\n",
      "creating files\n",
      "creating files\n",
      "creating files\n",
      "creating files\n",
      "creating files\n",
      "creating files\n",
      "creating files\n",
      "creating files\n",
      "creating files\n",
      "creating files\n",
      "creating files\n",
      "creating files\n",
      "HLS.S30.T14RNS.2020142.v1.4\n",
      "invalid\n",
      "HLS.S30.T14RNS.2020145.v1.4\n",
      "valid\n",
      "HLS.S30.T14RNS.2020145.v1.4\n",
      "creating files\n",
      "creating files\n",
      "creating files\n",
      "creating files\n",
      "creating files\n",
      "creating files\n",
      "creating files\n",
      "creating files\n",
      "creating files\n",
      "creating files\n",
      "creating files\n",
      "creating files\n",
      "creating files\n",
      "creating files\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[109], line 16\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalid\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;28mprint\u001b[39m(image_id)\n\u001b[1;32m---> 16\u001b[0m     \u001b[43mconvert_hdf2tiffs_mine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhdf_dir\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mimage_id\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m.hdf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtiff_dir\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtiff_dir_rpj\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdelete_original_tiff\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     17\u001b[0m     image_index\u001b[38;5;241m.\u001b[39mat[k, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconvert_to_tif\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "Cell \u001b[1;32mIn[107], line 109\u001b[0m, in \u001b[0;36mconvert_hdf2tiffs_mine\u001b[1;34m(hdf_path, dstdir, dstdir_rpj, bands, max_cloud_coverage, gdal_translate_options, delete_original_tiff)\u001b[0m\n\u001b[0;32m    107\u001b[0m dst_rpj\u001b[38;5;241m.\u001b[39mparent\u001b[38;5;241m.\u001b[39mmkdir(exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, parents\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    108\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 109\u001b[0m     \u001b[43msubprocess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcmd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshell\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    110\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m    111\u001b[0m     log\u001b[38;5;241m.\u001b[39mexception(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mERROR DURING REPROJECTION WITH CMD: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcmd\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)   \n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\envs\\IBM_CDL\\lib\\subprocess.py:364\u001b[0m, in \u001b[0;36mcheck_call\u001b[1;34m(*popenargs, **kwargs)\u001b[0m\n\u001b[0;32m    354\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcheck_call\u001b[39m(\u001b[38;5;241m*\u001b[39mpopenargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    355\u001b[0m     \u001b[38;5;124;03m\"\"\"Run command with arguments.  Wait for command to complete.  If\u001b[39;00m\n\u001b[0;32m    356\u001b[0m \u001b[38;5;124;03m    the exit code was zero then return, otherwise raise\u001b[39;00m\n\u001b[0;32m    357\u001b[0m \u001b[38;5;124;03m    CalledProcessError.  The CalledProcessError object will have the\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    362\u001b[0m \u001b[38;5;124;03m    check_call([\"ls\", \"-l\"])\u001b[39;00m\n\u001b[0;32m    363\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 364\u001b[0m     retcode \u001b[38;5;241m=\u001b[39m call(\u001b[38;5;241m*\u001b[39mpopenargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    365\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m retcode:\n\u001b[0;32m    366\u001b[0m         cmd \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124margs\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\envs\\IBM_CDL\\lib\\subprocess.py:347\u001b[0m, in \u001b[0;36mcall\u001b[1;34m(timeout, *popenargs, **kwargs)\u001b[0m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Popen(\u001b[38;5;241m*\u001b[39mpopenargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;28;01mas\u001b[39;00m p:\n\u001b[0;32m    346\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 347\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    348\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m:  \u001b[38;5;66;03m# Including KeyboardInterrupt, wait handled that.\u001b[39;00m\n\u001b[0;32m    349\u001b[0m         p\u001b[38;5;241m.\u001b[39mkill()\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\envs\\IBM_CDL\\lib\\subprocess.py:1209\u001b[0m, in \u001b[0;36mPopen.wait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m   1207\u001b[0m     endtime \u001b[38;5;241m=\u001b[39m _time() \u001b[38;5;241m+\u001b[39m timeout\n\u001b[0;32m   1208\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1209\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_wait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1210\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[0;32m   1211\u001b[0m     \u001b[38;5;66;03m# https://bugs.python.org/issue25942\u001b[39;00m\n\u001b[0;32m   1212\u001b[0m     \u001b[38;5;66;03m# The first keyboard interrupt waits briefly for the child to\u001b[39;00m\n\u001b[0;32m   1213\u001b[0m     \u001b[38;5;66;03m# exit under the common assumption that it also received the ^C\u001b[39;00m\n\u001b[0;32m   1214\u001b[0m     \u001b[38;5;66;03m# generated SIGINT and will exit rapidly.\u001b[39;00m\n\u001b[0;32m   1215\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\envs\\IBM_CDL\\lib\\subprocess.py:1490\u001b[0m, in \u001b[0;36mPopen._wait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m   1487\u001b[0m     timeout_millis \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(timeout \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1000\u001b[39m)\n\u001b[0;32m   1488\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturncode \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1489\u001b[0m     \u001b[38;5;66;03m# API note: Returns immediately if timeout_millis == 0.\u001b[39;00m\n\u001b[1;32m-> 1490\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43m_winapi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mWaitForSingleObject\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1491\u001b[0m \u001b[43m                                         \u001b[49m\u001b[43mtimeout_millis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1492\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;241m==\u001b[39m _winapi\u001b[38;5;241m.\u001b[39mWAIT_TIMEOUT:\n\u001b[0;32m   1493\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m TimeoutExpired(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs, timeout)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "cloud_threshold = 80\n",
    "valid_months = [4, 5, 6, 7, 8, 9]\n",
    "for k in range(len(image_index)):\n",
    "    img_data = image_index.iloc[k]\n",
    "    # Loop through images and check cloud cover\n",
    "    image_id = img_data['image_id']\n",
    "    print(image_id)\n",
    "    tile = img_data['tile']\n",
    "    date = img_data['date']\n",
    "    month = img_data['month']\n",
    "    cloud_cov = img_data['cloud_coverage']\n",
    "    \n",
    "    if((cloud_cov <= cloud_threshold) and (month in valid_months)):\n",
    "        print('valid')\n",
    "        print(image_id)\n",
    "        convert_hdf2tiffs_mine(Path(hdf_dir + image_id + '.hdf'), Path(tiff_dir), Path(tiff_dir_rpj), delete_original_tiff = False)\n",
    "        image_index.at[k, 'convert_to_tif'] = True\n",
    "    else:\n",
    "        print('invalid')\n",
    "    #    print(image_id)\n",
    "        image_index.at[k, 'convert_to_tif'] = False\n",
    "#     for cd, ct, cu in zip(candidate_dates, candidate_tiles, candidate_urls):\n",
    "#         print(cu)\n",
    "#         local_name = cu.split('/')[-1].replace(\"\\n\", \"\")\n",
    "        \n",
    "#         if local_name != \"HLS.S30.T14RNS.2020005.v1.4.hdf\":\n",
    "#             continue\n",
    "#        # print('test')\n",
    "#         #print(Path(hdf_dir+local_name))\n",
    "#         #print(Path(tiff_dir))\n",
    "#         convert_hdf2tiffs_mine(Path(hdf_dir+local_name), Path(tiff_dir), Path(tiff_dir_rpj), delete_original_tiff = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64823ba5-0b08-4038-8ab2-d3419e42c3c1",
   "metadata": {},
   "source": [
    "Create mask tif files (1 if valid data, 0 if not). Uses geojson (lat long) to mask. Does not crop. (WORKS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f764a06-ca08-490d-9804-17da61f68c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_index.head(25)\n",
    "image_index.to_csv(image_index_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e9c9262-9312-448e-8d84-d1cd5ca540e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for aoi_ind in range(nfeatures):\n",
    "#     aoi_row = aoi_src.iloc[aoi_ind]\n",
    "#     print(aoi_row)\n",
    "#     print(aoi_src.bounds)\n",
    "#     aoi_x = float(aoi_row.centroid_x)\n",
    "#     aoi_y = float(aoi_row.centroid_y)\n",
    "        \n",
    "#     # Identify what tile the burn scar is in\n",
    "#     s = (tile_x-aoi_x)**2+(tile_y-aoi_y)**2\n",
    "#     tname = tile_name[np.argmin(s)]\n",
    "#     print(s)\n",
    "#     print(tname)\n",
    "#     aoi_src.at[aoi_ind, 'tile'] = tname\n",
    "#     aoi_src.at[aoi_ind, 'chip_id'] = 'chip_' + str(aoi_ind)\n",
    "    \n",
    "#     # Subset potential images based on tile and date\n",
    "#     tile_mask = qtile == tname\n",
    "#     # Subset based on date\n",
    "#     date_mask1 = (qyear == yoi[0])\n",
    "#   #  diff = (qmonth-aoi_month)%12\n",
    "#    # date_mask2 = ((diff <= month_threshold[1]) & (diff >= month_threshold[0]))\n",
    "#     date_mask = date_mask1  #& date_mask2\n",
    "    \n",
    "#     mask = tile_mask & date_mask\n",
    "#     candidate_dates = qdate[mask]\n",
    "#     candidate_tiles = qtile[mask]\n",
    "#     candidate_urls = qurl[mask]\n",
    "#     # Loop through images and check cloud cover\n",
    "#     for cd, ct, cu in zip(candidate_dates, candidate_tiles, candidate_urls):\n",
    "#        # print(cu)\n",
    "#         local_name = cu.split('/')[-1].replace(\"\\n\", \"\")\n",
    "#         print(local_name)\n",
    "#         # if local_name != \"HLS.S30.T14RNS.2020345.v1.4.hdf\":\n",
    "#         #     continue\n",
    "#         # print('valid')\n",
    "#         # Download a candidate file to check cloud cover\n",
    "#         # try:\n",
    "#         #     urlreq.urlretrieve(cu, filename=hdf_dir+local_name)\n",
    "#         # except:\n",
    "#         #     continue\n",
    "    \n",
    "#         # Check cloud cover and spacial coverage\n",
    "#       #  try:\n",
    "#             # md = nasa_hls.get_metadata_from_hdf(hdf_dir+local_name, fields=['cloud_cover', 'spatial_coverage'])\n",
    "#             # if ((md[\"cloud_cover\"] < cldfrac_threshold) and (md['spatial_coverage'] > spacecov_threshold)):\n",
    "            \n",
    "#                 # Convert to a geotiff            \n",
    "#                 # nasa_hls.convert_hdf2tiffs(Path(hdf_dir+local_name), Path(tiff_dir))\n",
    "\n",
    "#             # Open the new geotiff\n",
    "#         date = datetime.strptime(cd, '%Y-%m-%d')\n",
    "#         doy = (date-datetime(date.year, 1, 1)).days+1\n",
    "#         fp = f'HLS.S30.T{ct}.{date.year}{doy:03d}.v1.4'\n",
    "#         src = rasterio.open(f'{tiff_dir}/{fp}__B04.tif')\n",
    "#         red = src.read(1)\n",
    "#         print(src)\n",
    "#         print(red)\n",
    "\n",
    "#         # Open NIR too to compute NDVI\n",
    "#         nir_src = rasterio.open(f'{tiff_dir}/{fp}__B08.tif')\n",
    "#         nir = nir_src.read(1)\n",
    "\n",
    "        \n",
    "#         qa_src = rasterio.open(f'{tiff_dir}/{fp}__QA.tif')\n",
    "#         qa = qa_src.read(1)\n",
    "        \n",
    "#         print(qa_src)\n",
    "#         print(qa)\n",
    "        \n",
    "#         # Compute NDVI\n",
    "#         red = np.clip(red/15000, 0, 1)\n",
    "#         nir = np.clip(nir/15000, 0, 1)\n",
    "#         ndvi = np.clip((nir-red)/(nir+red), 0, 1)\n",
    "\n",
    "#         # Convert burn scar shape to proper geometry\n",
    "#         aoi_shape = aoi_src.to_crs(src.crs).iloc[aoi_ind]\n",
    "\n",
    "#         print(aoi_shape)\n",
    "#         # Build the mask\n",
    "#         out_image, out_transform = rasterio.mask.mask(src, [aoi_shape.geometry], crop=False)\n",
    "#         print(np.min(out_image), np.max(out_image))\n",
    "#         out_image = np.clip(out_image, 0, 1)\n",
    "#         print(np.min(out_image), np.max(out_image))\n",
    "\n",
    "#         out_meta = src.meta\n",
    "#         out_meta.update({'driver':'GTiff', 'height':out_image.shape[1],\n",
    "#             'width':out_image.shape[2], 'transform':out_transform})\n",
    "\n",
    "#         with rasterio.open(f'{mask_dir}/{fp}.mask.tif', 'w', **out_meta) as dest:\n",
    "#             dest.write(out_image)\n",
    "\n",
    "#         # Plot the original and mask\n",
    "#         fig, [ax1, ax2] = pp.subplots(ncols=2)\n",
    "#         ax1.imshow(ndvi, cmap='YlGn', vmin=0, vmax=1)\n",
    "#         ax2.imshow(np.squeeze(out_image), cmap='Greys')\n",
    "#         pp.savefig(f'{mask_dir}/{fp}.mask.jpg')\n",
    "#         pp.close()\n",
    "\n",
    "#         # Save in index file\n",
    "# #         fn_index = open(index_file, 'a')\n",
    "# #         fn_index.write(f'\\n{aoi_row.Event_ID},{aoi_row.Incid_Name},{aoi_row.Ig_Date},{cd},{aoi_row.BurnBndAc},{ct},{fp}.mask.tif')\n",
    "# #         fn_index.close()\n",
    "\n",
    "# #         # Move onto the next burn scar\n",
    "# #         del src\n",
    "# #         del nir_src\n",
    "#        # break\n",
    "#         # except:\n",
    "#         #     print('error')\n",
    "#         #     continue"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
